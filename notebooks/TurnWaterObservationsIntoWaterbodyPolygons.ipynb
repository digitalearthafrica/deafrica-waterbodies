{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57ca7a3-711d-424e-8ff6-8bf7b30d2370",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Turn water observations into waterbody polygons\n",
    "\n",
    "* **Products used:** \n",
    "[wofs_ls_summary_alltime](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime)\n",
    "* **Special requirements:** \n",
    "This notebook requires the [python_geohash](https://pypi.org/project/python-geohash/) library. You can install it locally by using `python -m pip install python-geohash`.\n",
    "* **Prerequisites:** \n",
    "    * A coastline polygon to filter out polygons generated from ocean pixels.\n",
    "        * Variable name: `land_sea_mask_fp`\n",
    "        * Here we have used the [Marine Regions Global Oceans and Seas v01 dataset](https://www.marineregions.org/sources.php#goas).\n",
    "* **Optional prerequisites:**\n",
    "    * River line dataset for filtering out polygons comprised of river segments.\n",
    "        * Variable name: `major_rivers_fp`\n",
    "        * The option to filter out major rivers is provided, and so this dataset is optional if `filter_out_rivers = False`.\n",
    "        * We therefore turn this off during the production of the water bodies shapefile. \n",
    "    * Urban high rise polygon dataset\n",
    "        * Variable name: `urban_mask_fp`, but this is optional and can be skipped by setting `filter_out_urban_areas = False`.\n",
    "        * WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This results in 'waterbodies' around these misclassified shadows in capital cities. If you are not using WOfS for your analysis, you may choose to set `filter_out_urban_areas = False`.\n",
    "        * Here we haved generated a polygon dataset to act as our `urban_mask` by thresholding the High Resolution Population Density Maps dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551eb1a-fc0e-4242-8640-073922f3dd9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "Water is among one the most precious natural resources and is essential for the survival of life on Earth. For many countries in Africa, the scarcity of water is both an economic and social issue. Water is required not only for consumption but for industries and environmental ecosystems to function and flourish. \n",
    "\n",
    "With the demand for water increasing, there is a need to better understand our water availability to ensure we are managing our water resources effectively and efficiently.  \n",
    "\n",
    "Digital Earth Africa (DE Africa)'s [Water Observations from Space (WOfS) dataset](https://docs.digitalearthafrica.org/en/latest/data_specs/Landsat_WOfS_specs.html), provides a water classified image of Africa approximately every 16 days. These individual water observations have been combined into a [WOfS All-Time Summary](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime) product, which calculates the frequency of wet observations (compared against all clear observations of that pixel), over the full 30-plus years satellite archive. \n",
    "\n",
    "The WOfS All-Time Summary product provides valuable insights into the persistence of water across the African landscape on a pixel by pixel basis. While knowing the wet history of a single pixel within a waterbody is useful, it is more useful to be able to map the whole waterbody as a single object. \n",
    "\n",
    "This notebook demonstrates a workflow for mapping waterbodies across Africa as polygon objects. This workflow has been used to produce **DE Africa Waterbodies**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcb737-9bce-4943-9175-13e1682171ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Description\n",
    "This code follows the following workflow:\n",
    "\n",
    "* Load the required python packages\n",
    "* Load the required functions\n",
    "* Set your chosen analysis parameters:\n",
    "    * set up some file names for the inputs and outputs\n",
    "    * create a datacube query object\n",
    "    * set the analysis region\n",
    "    * wetness threshold/s\n",
    "    * min/max waterbody size\n",
    "    * minimum number of valid observations\n",
    "    * read in a land/sea mask\n",
    "    * optional flag to filter out waterbodies that intersect with major rivers\n",
    "        * if you set this flag you will need to provide a dataset to do the filtering\n",
    "    * read in an urban mask\n",
    "* Generate the first temporary polygon set:\n",
    "  * For each tile:\n",
    "    * Load the WOfS All Time Summary Dataset\n",
    "    * Keep only pixels observed at least x times\n",
    "    * Keep only pixels identified as wet at least x% of the time\n",
    "        * Here the code can take in two wetness thresholds, to produce two initial temporary polygon files.\n",
    "    * Convert the raster data into polygons\n",
    "    * Append the polygon set to a temporary shapefile\n",
    "* Remove artificial polygon borders created at tile boundaries by merging polygons that intersect across tile boundaries\n",
    "* Filter the combined polygon dataset (note that this step happens after the merging of tile boundary polygons to ensure that artifacts are not created by part of a polygon being filtered out, while the remainder of the polygon that sits on a separate tile is treated differently).\n",
    "    * Filter the polygons based on area / size\n",
    "    * Remove polygons that intersect with Africa's coastline\n",
    "    * Remove erroneous 'water' polygons within high-rise CBD areas\n",
    "    * Combine the two generated wetness thresholds (optional)\n",
    "    * Optional filtering for proximity to major rivers  \n",
    "* Save out the final polygon set to a shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfaf06-55d3-4a99-a8b3-08f1e607a524",
   "metadata": {},
   "source": [
    "## Load python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4662e58-833b-4bce-8479-fb7d1575e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install the local waterbodies functions\n",
    "!python -m pip install ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eace02f-97fc-40d6-a66d-6de98014c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil  #\n",
    "import pyproj\n",
    "import fiona\n",
    "import shapely\n",
    "import datetime\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import geohash as gh\n",
    "import logging\n",
    "\n",
    "import datacube\n",
    "from datacube.utils import geometry\n",
    "\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "from deafrica_tools.spatial import xr_vectorize, xr_rasterize\n",
    "\n",
    "from deafrica_waterbodies.cli.logs import logging_setup\n",
    "from deafrica_waterbodies.cli.io import write_waterbodies_to_file\n",
    "# from deafrica_waterbodies.cli.group_options import MutuallyExclusiveOption\n",
    "\n",
    "from deafrica_waterbodies.waterbodies.polygons.attributes import add_timeseries_attribute, add_area_and_perimeter_attributes, assign_unique_ids\n",
    "from deafrica_waterbodies.waterbodies.polygons.make_polygons import get_product_tiles, get_waterbodies, get_polygons_using_thresholds, merge_polygons_at_tile_boundary, filter_waterbodies, check_wetness_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca687c0-9448-4489-af86-5a84e0f02215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efcfafd8-3585-483f-9e6a-23160f086cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_setup(True)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae5375-e7e3-48a7-abbf-963d0a95edf9",
   "metadata": {},
   "source": [
    "## Define Analysis Parameters\n",
    "\n",
    "The following section walks you through the analysis parameters you will need to set for this workflow. Each section describes the parameter, how it is used, and what value was used for the DE Africa Waterbodies product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289de1c-2f87-4c6b-8316-966c5c382946",
   "metadata": {},
   "source": [
    "### Set up some file names for the inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ef421b-cec1-4433-8d33-d53e2461054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the outputs.\n",
    "output_dir = \"/home/jovyan/Data/Waterbodies/OutputDatasets\"\n",
    "output_dir_fp = Path(output_dir)\n",
    "os.makedirs(output_dir_fp, exist_ok=True)\n",
    "\n",
    "# Set up some filenames to use.\n",
    "base_filename = \"ContinentalWaterbodies\"\n",
    "base_filename_fp = output_dir_fp / base_filename\n",
    "\n",
    "os.makedirs(base_filename_fp, exist_ok=True)\n",
    "\n",
    "# Putting this here to allow for testing of both methods\n",
    "# to handle large polygons. \n",
    "#handle_large_polygons = 'erode-dilate-v1'\n",
    "handle_large_polygons = 'erode-dilate-v2'\n",
    "#handle_large_polygons = 'nothing'\n",
    "\n",
    "# The name and filepath of the first temporary polygon dataset.\n",
    "waterbodies_shapefile_temp = base_filename_fp / \"temp\"\n",
    "# The filepath for the location of temp files during the code run.\n",
    "waterbodies_shapefile_merged = base_filename_fp / \"merged\"\n",
    "# The name and filepath of the outputs following the filtering steps\n",
    "waterbodies_shapefile_filtered = f\"filtered_{handle_large_polygons.replace('-','_')}\"\n",
    "# The name and file path of the final, completed waterbodies shapefile\n",
    "waterbodies_shapefile_final = f\"final_{handle_large_polygons.replace('-','_')}\"\n",
    "\n",
    "# File extension to use for outputs , either .shp (ESRI Shapefile) or .geojson (GeoJSON)\n",
    "file_extension = \".shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20236caf-b0eb-4e1e-8c1c-c6a318f5edb3",
   "metadata": {},
   "source": [
    "### Define parameters to use when loading data from the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed4bd317-4be1-4fd1-939b-9b8f1d4fd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_chunks = {'x': 3500, 'y': 3500, 'time': 1}\n",
    "# Resolution of the WOfS datasets.\n",
    "resolution = (-30, 30)\n",
    "# CRS to work with for all files.\n",
    "crs = \"EPSG:6933\"\n",
    "\n",
    "# Create a datacube query.\n",
    "query = dict(dask_chunks=dask_chunks, resolution=resolution, output_crs=crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64866886-3224-452e-b07e-966ac709cd04",
   "metadata": {},
   "source": [
    "### Set the analysis region\n",
    "If you would like to perform the analysis for all of Africa, using the published WOfS All-time Summary, set `all_of_africa = True`. If you set the flag `all_of_africa` to `False`, you will need to provide either a latitude and longitude range covering the area of interest, a path to the shapefile / GeoJSON defining the area of interest, or a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4294ec5c-53f3-455a-b650-975149682a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-03 02:10:23,403] {make_polygons.py:81} INFO - Getting all wofs_ls_summary_alltime regions...\n",
      "[2023-10-03 02:10:23,404] {make_polygons.py:93} INFO - 4456 wofs_ls_summary_alltime tiles found.\n"
     ]
    }
   ],
   "source": [
    "all_of_africa = True\n",
    "\n",
    "if not all_of_africa:\n",
    "    #\"\"\"\n",
    "    # Load a shapefile or GeoJSON file for the area of interest:\n",
    "    basin = define_area(\n",
    "        vector_path=\n",
    "        \"/home/jovyan/Data/Waterbodies/InputDatasets/SenegalBasin.geojson\")\n",
    "    geopolygon = geometry.Geometry(basin.features[0][\"geometry\"],\n",
    "                                   crs=\"epsg:4326\")\n",
    "    #\"\"\"\n",
    "    \"\"\"\n",
    "    # Use the section below if you would like to define the area of interest using a bounding box.\n",
    "    # Bounding box for a section of Bukama in the Democratic Republic of Congo\n",
    "    # that covers the lakes Kabwe, Kabele and Mulenda.\n",
    "    bbox = (25.752395, -9.267306, 26.189124, -8.610346)\n",
    "    left, bottom, right, top = bbox\n",
    "    geopolygon = geometry.box(left, bottom, right, top, crs=\"EPSG:4326\")\n",
    "    \"\"\"\n",
    "elif all_of_africa:\n",
    "    geopolygon = None\n",
    "# Generate the tiles to be used in this workflow.\n",
    "tiles = get_product_tiles(product=\"wofs_ls_summary_alltime\", aoi_gdf=geopolygon)\n",
    "\n",
    "# Uncomment the section below to output the tiles.\n",
    "#tiles_output_fp = base_filename_fp/f\"tiles{file_extension}\"\n",
    "#tiles.to_file(tiles_output_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2c027-1c6f-4fda-9343-b3d3a78328b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='wetnessthreshold'></a>\n",
    "### How frequently wet does a pixel need to be to be included?\n",
    "The value/s set here will be the minimum frequency (as a decimal between 0 and 1) that you want water to be detected across all analysis years before it is included. \n",
    "\n",
    "E.g. If this was set to 0.10, any pixels that are wet *at least* 10% of the time across all valid observations will be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "Following the exploration of an appropriate wetness threshold for DE Africa Waterbodies [see here]( Add-link-to-notebook-showing-threshold-sensisitivity-analysis), we choose to set two thresholds here. The code is set up to loop through both wetness thresholds, and to write out two temporary shapefiles. These two shapefiles with two separate thresholds are then used together to combine polygons from both thresholds later on in the workflow.\n",
    "\n",
    "Polygons identified by the secondary threshold that intersect with the polygons generated by the primary threshold will be extracted, and included in the final polygon dataset. This means that the **location** of polygons is set by the primary threshold, but the **shape** of these polygons is set by the secondary threshold.\n",
    "\n",
    "Threshold values need to be provided as a list of either one or two floating point numbers. If one number is provided, then this will be used to generate the initial polygon dataset. If two thresholds are entered, the **first number becomes the secondary threshold, and the second number becomes the primary threshold**. If more than two numbers are entered, the code will generate an error below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e81f9ff-4000-47d0-977c-7a15af0514c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will be running a hybrid wetness threshold. \\n**You have set 0.1 as the primary threshold, which will define the location of the waterbody polygons \\n with 0.05 set as the supplementary threshold, which will define the extent/shape of the waterbody polygons.**'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the smaller threshold is first in the list if using 2 thresholds.\n",
    "secondary_threshold = 0.05\n",
    "primary_threshold = 0.1\n",
    "minimum_wet_thresholds = [secondary_threshold, primary_threshold]\n",
    "\n",
    "check_wetness_thresholds(minimum_wet_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c5ac3-d193-416f-9839-936dad86f806",
   "metadata": {},
   "source": [
    "<a id='size'></a>\n",
    "\n",
    "### How big/small should the polygons be?\n",
    "This filtering step can remove very small and/or very large waterbody polygons. The size listed here is in m<sup>2</sup>. A single pixel in Landsat data is 30 m x 30 m = 900 m<sup>2</sup>. \n",
    "\n",
    "**MinSize**\n",
    "\n",
    "E.g. A minimum size of 9000 m<sup>2</sup> means that polygons need to be at least 10 pixels to be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "**MaxSize**\n",
    "\n",
    "E.g. A maximum size of 1 000 000 m<sup>2</sup> means that you only want to consider polygons less than 1 km<sup>2</sup>. If you don't want to use this filter, set this number to `math.inf`. \n",
    "\n",
    "*NOTE: if you are doing this analysis for all of Africa, very large polygons will be generated offshore, in the steps prior to filtering by the specified `land_sea_mask`. For this reason, we have used a `max_polygon_size` = Area of Lake Victoria (the largest lake in Africa). This will remove the huge ocean polygons, but keep large inland waterbodies that we want to map.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d206fa47-6d55-43f3-a7af-98f98b28ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_polygon_size = 4500  # 5 pixels\n",
    "max_polygon_size = math.inf  #59947000000 approx area of Lake Victoria 59947 sq. km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a9d28-8b7e-4453-a20a-b873455ffdb6",
   "metadata": {},
   "source": [
    "### Filter results based on number of valid observations\n",
    "\n",
    "The total number of valid WOfS observations for each pixel varies depending on the frequency of clouds and cloud shadow, the proximity to high slope and terrain shadow, and the seasonal change in solar angle. \n",
    "\n",
    "The `count_clear` parameter within the [`wofs_ls_summary_alltime`](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime) data provides a count of the number of valid observations each pixel recorded over the analysis period. We can use this parameter to mask out pixels that were infrequently observed. \n",
    "If this mask is not applied, pixels that were observed only once could be included if that observation was wet (i.e. a single wet observation means the calculation of the frequency statistic would be (1 wet observation) / (1 total observation) = 100% frequency of wet observations).\n",
    "\n",
    "Here we set the minimum number of observations to be 128 (roughly 4 per year over our 32 year analysis). Note that this parameter does not specify the timing of these observations, but rather just the **total number of valid observations** (observed at any time of the year, in any year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22e21ec-c9a1-4450-bcf9-999f521ddd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "this_year = datetime.datetime.now().year\n",
    "start_year_wofs_dataset = 1984\n",
    "min_valid_observations_yearly = 4\n",
    "\n",
    "no_of_years = this_year - start_year_wofs_dataset\n",
    "\n",
    "#min_valid_observations = min_valid_observations_yearly * no_of_years\n",
    "\n",
    "min_valid_observations = 128\n",
    "\n",
    "print(min_valid_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e49a5a-978b-466b-84b8-31578121b043",
   "metadata": {},
   "source": [
    "<a id='coastline'></a>\n",
    "### Read in a land/sea mask\n",
    "\n",
    "You can choose which land/sea mask you would like to use to mask out ocean polygons, depending on how much coastal water you would like in the final product. \n",
    "\n",
    "We use the [Marine Regions Global Oceans and Seas v01 dataset](https://www.marineregions.org/sources.php#goas). Any polygons that intersect with this mask are filtered out, i.e. if a polygon identified within our workflow overlaps with this coastal mask by even a single pixel, it will be discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87eae51e-87bf-483d-82cb-a4da43f4ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out_ocean_polygons = True\n",
    "\n",
    "if filter_out_ocean_polygons:\n",
    "    land_sea_mask_fp = \"/home/jovyan/Data/Waterbodies/InputDatasets/goas_v01.gpkg\"\n",
    "    #land_sea_mask = gpd.read_file(land_sea_mask_fp).to_crs(crs)\n",
    "else: \n",
    "    land_sea_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db9fdc-fea1-45e9-b8e6-4f9153151cdb",
   "metadata": {},
   "source": [
    "<a id='rivers'></a>\n",
    "### Do you want to filter out polygons that intersect with major rivers?\n",
    "\n",
    "This filtering step is done to remove river segments from the polygon dataset. \n",
    "Set the filepath to the dataset you would wish to use in the `major_rivers_fp` variable. The dataset needs to be a vector dataset, and [able to be read in by the fiona python library](https://fiona.readthedocs.io/en/latest/fiona.html#fiona.open).\n",
    "\n",
    "Note that we reproject this dataset to the CRS specified in the variable `crs` to match the coordinate reference system of the WOfS data we use. A list of epsg code [can be found here](https://spatialreference.org/ref/epsg/).\n",
    "\n",
    "If you don't want to filter out polygons that intersect with rivers, set this parameter to `False`.\n",
    "\n",
    "**Note that for the DE Africa Water Body Polygon dataset, we set this filter to False (`filter_out_rivers = False`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd024b4-270f-4c8e-92ab-7ba52a46b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out_major_rivers_polygons = False\n",
    "\n",
    "if filter_out_major_rivers_polygons:\n",
    "    # Insert path to the dataset location.\n",
    "    major_rivers_mask_fp = \"\"\n",
    "    #major_rivers = gpd.GeoDataFrame.from_file(major_rivers_fp)\n",
    "    #major_rivers = major_rivers.to_crs(crs)\n",
    "else:\n",
    "    major_rivers_mask_fp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bd06f-cd83-4e53-a1bc-c213fd496206",
   "metadata": {},
   "source": [
    "<a id='Urban'></a>\n",
    "\n",
    "### Read in a mask for high-rise CBDs\n",
    "\n",
    "WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This results in 'waterbodies' around these misclassified shadows in capital cities. \n",
    "\n",
    "To address this problem, we use the High Resolution Population Density Maps dataset to define a spatial footprint for Africa's CBD areas. The theory of using this dataset is that high-rises have a high population density (population count per area). Therefore pixels in the HRPDM dataset with a higher general population density than the specified threshold are vectorized and used as our CBD filter.\n",
    "\n",
    "If you are not using WOfS for your analysis, you may choose to set `filter_out_urban_areas = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6340d819-aa2e-46b3-b35c-08596477c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out_urban_polygons = False\n",
    "\n",
    "if filter_out_urban_polygons:\n",
    "    urban_mask_fp = \"\"\n",
    "else:\n",
    "    urban_mask_fp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb76c9-9c79-4a0d-a2a6-f424f26fcf16",
   "metadata": {},
   "source": [
    "## Generate the first temporary polygon dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439148e2-bff1-44e3-9397-05d921b5cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"Generating the first temporary set of waterbody polygons.\")\n",
    "temp_primary, temp_secondary = get_polygons_using_thresholds(\n",
    "    input_gdf=tiles,\n",
    "    dask_chunks=dask_chunks,\n",
    "    resolution=resolution,\n",
    "    output_crs=crs,\n",
    "    min_valid_observations=min_valid_observations,\n",
    "    primary_threshold=primary_threshold,\n",
    "    secondary_threshold=secondary_threshold,\n",
    "    temp_dir=waterbodies_shapefile_temp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28d966-b0dc-4cf5-a111-3707b2787116",
   "metadata": {},
   "source": [
    "### Check for completed tiles and rerun if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58581751-9084-417d-a5d1-5867f1b3a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Check for which tiles have already been completed\n",
    "\n",
    "# import re\n",
    "\n",
    "# # Regular expression pattern\n",
    "# pattern = r'x\\d{3}y\\d{3}'\n",
    "\n",
    "# temp_folder = waterbodies_shapefile_temp / \"0-0-1\" / \"shapefile\"\n",
    "\n",
    "# temp_primary = temp_folder.glob(f'temp_{primary_threshold}*.geojson')\n",
    "# temp_secondary = temp_folder.glob(f'temp_{secondary_threshold}*.geojson')\n",
    "\n",
    "# temp_primary_tiles = [re.search(pattern, temp_primary_file.name).group(0) for temp_primary_file in temp_primary]\n",
    "# temp_secondary_tiles = [re.search(pattern, temp_secondary_file.name).group(0) for temp_secondary_file in temp_secondary]\n",
    "\n",
    "# print(len(temp_primary_names))\n",
    "# print(len(temp_secondary_names))\n",
    "\n",
    "# completed_tiles = [tile_number for tile_number in temp_secondary_names if tile_number in temp_primary_names]\n",
    "\n",
    "# tile_completion_status = tiles[\"label\"].isin(completed_tiles)\n",
    "\n",
    "# remaining_tiles = tiles[~tile_completion_status]\n",
    "\n",
    "# ## Can then rerun remaining tiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e017c7-2242-422b-8f58-eebb3892697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load some tiles and merge them if not alread in memory\n",
    "\n",
    "# test_tiles = [f\"x{xtile}y0{ytile}\" for xtile in range(200, 210) for ytile in range(70, 80)]\n",
    "\n",
    "# test_primary_files = [waterbodies_shapefile_temp / \"0-0-1\" / \"shapefile\" / f\"temp_{primary_threshold}_{tile}.geojson\" for tile in test_tiles]\n",
    "# test_secondary_files = [waterbodies_shapefile_temp / \"0-0-1\" / \"shapefile\" / f\"temp_{secondary_threshold}_{tile}.geojson\" for tile in test_tiles]\\\n",
    "\n",
    "# primary_data_list = []\n",
    "# with fiona.Env(OGR_GEOJSON_MAX_OBJ_SIZE=2000):\n",
    "#     for dataset in list(test_primary_files):\n",
    "#         df = gpd.read_file(dataset)\n",
    "#         primary_data_list.append(df)\n",
    "    \n",
    "# temp_primary = pd.concat(primary_data_list)\n",
    "\n",
    "# secondary_data_list = []\n",
    "# with fiona.Env(OGR_GEOJSON_MAX_OBJ_SIZE=2000):\n",
    "#     for dataset in list(test_secondary_files):\n",
    "#         df = gpd.read_file(dataset)\n",
    "#         secondary_data_list.append(df)\n",
    "        \n",
    "# temp_secondary = pd.concat(secondary_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323aefd0-5e66-4808-8ab0-580c57c26bff",
   "metadata": {},
   "source": [
    "## Merge polygons that have an edge at a tile boundary\n",
    "\n",
    "Now that we have all of the polygons across our whole region of interest, we need to check for artifacts in the data caused by tile boundaries.\n",
    "\n",
    "We have created a GeoDataFrame `buffered_30m_tiles`, that consists of the tile boundaries, plus a 1 pixel (30 m) buffer. This GeoDataFrame will help us to find any polygons that have a boundary at the edge of a tile. We can then find where polygons touch across this boundary, and join them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df829f4-c5c5-4edb-bd37-00258472774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"Merging polygons at tile boundaries...\")\n",
    "merged_temp_primary = merge_polygons_at_tile_boundary(input_polygons=temp_primary, tiles=tiles.to_crs(crs))\n",
    "\n",
    "merged_temp_primary.to_file(\"/home/jovyan/Data/Waterbodies/OutputDatasets/ContinentalWaterbodies/temp/0-0-1/shapefile/temp_merged_primary.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e381ee7-9cf1-49c6-83fe-e7e0ee08698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"Merging polygons at tile boundaries...\")\n",
    "merged_temp_secondary = merge_polygons_at_tile_boundary(input_polygons=temp_secondary, tiles=tiles.to_crs(crs))\n",
    "\n",
    "merged_temp_secondary.to_file(\"/home/jovyan/Data/Waterbodies/OutputDatasets/ContinentalWaterbodies/temp/0-0-1/shapefile/temp_merged_secondary.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20febf05-883e-4678-b75e-60aecf3666e5",
   "metadata": {},
   "source": [
    "<a id='Filtering'></a>\n",
    "\n",
    "## Filter the merged polygons by:\n",
    "- **Area:**\n",
    "Based on the `min_polygon_size` and `max_polygon_size` parameters set [here](#size).\n",
    "- **Coastline:**\n",
    "Using the `land_sea_mask` dataset loaded [here](#coastline).\n",
    "- **CBD location (optional):**\n",
    "Using the `urban_mask` dataset loaded [here](#Urban).\n",
    "- **Wetness thresholds:**\n",
    "Here we apply the hybrid threshold described [here](#wetnessthreshold)\n",
    "- **Intersection with rivers (optional):**\n",
    "Using the `major_rivers` dataset loaded [here](#rivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d93e2-4ff5-4de3-933f-360b62b12295",
   "metadata": {},
   "source": [
    "\n",
    "### Dividing up very large polygons\n",
    "\n",
    "The size of polygons is determined by the contiguity of waterbody pixels through the landscape. This can result in very large polygons, e.g. where rivers are wide and unobscured by trees, or where waterbodies are connected to rivers or neighbouring waterbodies. \n",
    "\n",
    "We can break too large polygons into smaller, more useful polygons by applying the [Polsby-Popper test (1991)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2936284). The Polsby-Popper test is an assessment of the 'compactness' of a polygon. This method was originally developed to test the shape of congressional and state legislative districts, to prevent gerrymandering. \n",
    "\n",
    "The Polsby-Popper test examines the ratio between the area of a polygon, and the area of a circle equal to the perimeter of that polygon. The result falls between 0 and 1, with values closer to 1 being assessed as more compact.\n",
    "\n",
    "\\begin{align*}\n",
    "PPtest = \\frac{polygon\\ area * 4\\pi}{polygon\\ perimeter^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa222f3-5480-4fb7-92cc-616fb8acaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_test_threshold = 0.005\n",
    "\n",
    "_log.info(\"Filtering waterbodies...\")\n",
    "filtered_polygons = filter_waterbodies(\n",
    "    primary_threshold_polygons=merged_temp_primary,\n",
    "    secondary_threshold_polygons=merged_temp_secondary,\n",
    "    min_polygon_size=min_polygon_size,\n",
    "    max_polygon_size=max_polygon_size,\n",
    "    filter_out_ocean_polygons=filter_out_ocean_polygons,\n",
    "    land_sea_mask_fp=land_sea_mask_fp,\n",
    "    filter_out_major_rivers_polygons=filter_out_major_rivers_polygons,\n",
    "    major_rivers_mask_fp=major_rivers_mask_fp,\n",
    "    filter_out_urban_polygons=filter_out_urban_polygons,\n",
    "    urban_mask_fp=urban_mask_fp,\n",
    "    handle_large_polygons=handle_large_polygons,\n",
    "    pp_test_threshold=pp_test_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138497a-d8a9-4c38-bbf3-cbcd399297a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43130c4-43ec-4393-b4bd-d8568a78786e",
   "metadata": {},
   "source": [
    "### Generate a unique ID for each polygon\n",
    "\n",
    "A unique identifier is required for every polygon to allow it to be referenced. The naming convention for generating unique IDs here is the [geohash](geohash.org).\n",
    "\n",
    "A Geohash is a geocoding system used to generate short unique identifiers based on latitude/longitude coordinates. It is a short combination of letters and numbers, with the length of the string a function of the precision of the location. The methods for generating a geohash are outlined [here - yes, the official documentation is a wikipedia article](https://en.wikipedia.org/wiki/Geohash).\n",
    "\n",
    "Here we use the python package `python-geohash` to generate a geohash unique identifier for each polygon. We use `precision = 9` geohash characters, which represents an on the ground accuracy of <20 metres. This ensures that the precision is high enough to differentiate between waterbodies located next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b949bc-6cb0-4a55-bb32-9a5d779d2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_polygons_with_unique_ids = assign_unique_ids(filtered_polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac652ae0-3e5b-4b99-88fb-e99d5b63a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_version = \"0.0.1\"\n",
    "output_bucket_name = \"deafrica-waterbodies-dev\"\n",
    "output_file_name = f\"filtered_{handle_large_polygons.replace('-','_')}\"\n",
    "output_file_type = \"GeoJSON\"\n",
    "\n",
    "write_waterbodies_to_file(\n",
    "    filtered_polygons_with_unique_ids,\n",
    "    product_version = product_version,\n",
    "    storage_location=\"local\", \n",
    "    output_bucket_name=output_bucket_name,\n",
    "    output_local_folder=base_filename_fp,\n",
    "    output_file_name=output_file_name,\n",
    "    output_file_type=output_file_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341074c-a532-420e-890a-565cf33ce5aa",
   "metadata": {},
   "source": [
    "### Final checks and recalculation of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213406c-e004-4e22-a832-b3fd3003914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_polygons = gpd.read_file(\"/home/jovyan/Data/Waterbodies/OutputDatasets/ContinentalWaterbodies/0-0-1/shapefile/filtered_erode_dilate_v2.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a680617-bebc-4aaf-8abf-3aa5126df3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_version = \"0.0.1\"\n",
    "# output_bucket_name = \"deafrica-waterbodies-dev\"\n",
    "\n",
    "waterbodies_gdf = add_area_and_perimeter_attributes(filtered_polygons)\n",
    "waterbodies_gdf = add_timeseries_attribute(waterbodies_gdf,\n",
    "                                           product_version,\n",
    "                                           output_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784b15f-4051-48df-ba58-3550e6dd2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out final results to file.\n",
    "final_output_fp = f\"final_{handle_large_polygons.replace('-','_')}\"\n",
    "\n",
    "# Extra step to ensure final output is in EPSG:4326\n",
    "filtered_polygons_with_unique_ids_sorted = waterbodies_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "write_waterbodies_to_file(\n",
    "    filtered_polygons_with_unique_ids_sorted,\n",
    "    product_version = product_version,\n",
    "    storage_location=\"local\", \n",
    "    output_bucket_name=output_bucket_name,\n",
    "    output_local_folder=base_filename_fp,\n",
    "    output_file_name=final_output_fp,\n",
    "    output_file_type=output_file_type,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
