{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57ca7a3-711d-424e-8ff6-8bf7b30d2370",
   "metadata": {},
   "source": [
    "# Turn water observations into waterbody polygons\n",
    "\n",
    "* **Products used:** \n",
    "[wofs_ls_summary_alltime](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime)\n",
    "* **Special requirements:** \n",
    "This notebook requires the [python_geohash](https://pypi.org/project/python-geohash/) library. You can install it locally by using `python -m pip install python-geohash`.\n",
    "* **Prerequisites:** \n",
    "    * A coastline polygon to filter out polygons generated from ocean pixels.\n",
    "        * Variable name: `land_sea_mask_fp`\n",
    "        * Here we have used the [Marine Regions Global Oceans and Seas v01 dataset](https://www.marineregions.org/sources.php#goas).\n",
    "* **Optional prerequisites:**\n",
    "    * River line dataset for filtering out polygons comprised of river segments.\n",
    "        * Variable name: `major_rivers_fp`\n",
    "        * The option to filter out major rivers is provided, and so this dataset is optional if `filter_out_rivers = False`.\n",
    "        * We therefore turn this off during the production of the water bodies shapefile. \n",
    "    * Urban high rise polygon dataset\n",
    "        * Variable name: `urban_mask_fp`, but this is optional and can be skipped by setting `filter_out_urban_areas = False`.\n",
    "        * WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This results in 'waterbodies' around these misclassified shadows in capital cities. If you are not using WOfS for your analysis, you may choose to set `filter_out_urban_areas = False`.\n",
    "        * Here we haved generated a polygon dataset to act as our `urban_mask` by thresholding the High Resolution Population Density Maps dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551eb1a-fc0e-4242-8640-073922f3dd9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "Water is among one the most precious natural resources and is essential for the survival of life on Earth. For many countries in Africa, the scarcity of water is both an economic and social issue. Water is required not only for consumption but for industries and environmental ecosystems to function and flourish. \n",
    "\n",
    "With the demand for water increasing, there is a need to better understand our water availability to ensure we are managing our water resources effectively and efficiently.  \n",
    "\n",
    "Digital Earth Africa (DE Africa)'s [Water Observations from Space (WOfS) dataset](https://docs.digitalearthafrica.org/en/latest/data_specs/Landsat_WOfS_specs.html), provides a water classified image of Africa approximately every 16 days. These individual water observations have been combined into a [WOfS All-Time Summary](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime) product, which calculates the frequency of wet observations (compared against all clear observations of that pixel), over the full 30-plus years satellite archive. \n",
    "\n",
    "The WOfS All-Time Summary product provides valuable insights into the persistence of water across the African landscape on a pixel by pixel basis. While knowing the wet history of a single pixel within a waterbody is useful, it is more useful to be able to map the whole waterbody as a single object. \n",
    "\n",
    "This notebook demonstrates a workflow for mapping waterbodies across Africa as polygon objects. This workflow has been used to produce **DE Africa Waterbodies**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcb737-9bce-4943-9175-13e1682171ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Description\n",
    "This code follows the following workflow:\n",
    "\n",
    "* Load the required python packages\n",
    "* Load the required functions\n",
    "* Set your chosen analysis parameters:\n",
    "    * set up some file names for the inputs and outputs\n",
    "    * create a datacube query object\n",
    "    * set the analysis region\n",
    "    * wetness threshold/s\n",
    "    * min/max waterbody size\n",
    "    * minimum number of valid observations\n",
    "    * read in a land/sea mask\n",
    "    * optional flag to filter out waterbodies that intersect with major rivers\n",
    "        * if you set this flag you will need to provide a dataset to do the filtering\n",
    "    * read in an urban mask\n",
    "* Generate the first temporary polygon set:\n",
    "  * For each tile:\n",
    "    * Load the WOfS All Time Summary Dataset\n",
    "    * Keep only pixels observed at least x times\n",
    "    * Keep only pixels identified as wet at least x% of the time\n",
    "        * Here the code can take in two wetness thresholds, to produce two initial temporary polygon files.\n",
    "    * Convert the raster data into polygons\n",
    "    * Append the polygon set to a temporary shapefile\n",
    "* Remove artificial polygon borders created at tile boundaries by merging polygons that intersect across tile boundaries\n",
    "* Filter the combined polygon dataset (note that this step happens after the merging of tile boundary polygons to ensure that artifacts are not created by part of a polygon being filtered out, while the remainder of the polygon that sits on a separate tile is treated differently).\n",
    "    * Filter the polygons based on area / size\n",
    "    * Remove polygons that intersect with Africa's coastline\n",
    "    * Remove erroneous 'water' polygons within high-rise CBD areas\n",
    "    * Combine the two generated wetness thresholds (optional)\n",
    "    * Optional filtering for proximity to major rivers  \n",
    "* Save out the final polygon set to a shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfaf06-55d3-4a99-a8b3-08f1e607a524",
   "metadata": {},
   "source": [
    "## Load python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6ada7-7851-4398-8331-939503835316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install the python_geohash package.\n",
    "# !python -m pip install python_geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eace02f-97fc-40d6-a66d-6de98014c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil  #\n",
    "import pyproj\n",
    "import fiona\n",
    "import shapely\n",
    "import datetime\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import geohash as gh\n",
    "\n",
    "import datacube\n",
    "from datacube.utils import geometry\n",
    "\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "from deafrica_tools.spatial import xr_vectorize, xr_rasterize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e3429-7646-4269-afaf-d8e7d56f11e6",
   "metadata": {},
   "source": [
    "## Define functions required in the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb489351-db6b-4adf-939d-3bc0db605d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(aoi_geopolygon=None, all_of_africa=True, crs=\"EPSG:4326\"):\n",
    "    \"\"\"\n",
    "    Returns a GeoDataFrame of the tiles intersecting with the area of \n",
    "    interest Geometry object or all tiles/regions from the WOfS \n",
    "    All time summary product.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    aoi_geopolygon: datacube.utils.geometry._base.Geometry\n",
    "        Geometry object of the area of interest\n",
    "    all_of_africa : bool\n",
    "        Default True. If set to True, all the tiles present in the \n",
    "        WOfS All time summary product regions GeoJSON file will be loaded.\n",
    "        If set to True aoi_geopolygon must be set to None. \n",
    "    crs: str\n",
    "        CRS of the returned tiles GeoDataFrame. Defaults to \"EPSG:4326\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.geodataframe.GeoDataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    # Set important variables.\n",
    "    wofs_ls_summary_alltime_regions_url = \"https://explorer.digitalearth.africa/api/regions/wofs_ls_summary_alltime\"\n",
    "\n",
    "    # Convert the area of interest geopolygon to the specified crs.\n",
    "    aoi_geopolygon = aoi_geopolygon.to_crs(crs)\n",
    "\n",
    "    # Checks on arguments passed to function.\n",
    "    if not all_of_africa and aoi_geopolygon is None:\n",
    "        raise Exception(\n",
    "            \"Please pass a Geometry object to aoi_geopolygon if all_of_africa is set to False.\"\n",
    "        )\n",
    "    elif all_of_africa and aoi_geopolygon is not None:\n",
    "        raise Exception(\n",
    "            \"Please pass None to aoi_geopolygon if all_of_africa is set to True.\"\n",
    "        )\n",
    "\n",
    "    if all_of_africa and aoi_geopolygon is None:\n",
    "        tiles = gpd.read_file(wofs_ls_summary_alltime_regions_url).drop(\n",
    "            'count', axis=1).to_crs(crs)\n",
    "    elif not all_of_africa and aoi_geopolygon is not None:\n",
    "        africa_tiles = gpd.read_file(wofs_ls_summary_alltime_regions_url).drop(\n",
    "            'count', axis=1).to_crs(crs)\n",
    "\n",
    "        keep_tiles_list = []\n",
    "        for row in africa_tiles.itertuples():\n",
    "            row_geopolygon = geometry.Geometry(geom=row.geometry, crs=crs)\n",
    "            if geometry.intersects(row_geopolygon, aoi_geopolygon):\n",
    "                keep_tiles_list.append(row.region_code)\n",
    "\n",
    "        tiles = africa_tiles[africa_tiles['region_code'].isin(\n",
    "            keep_tiles_list)].reset_index(drop=True)\n",
    "\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e4638-494d-44d3-8740-5abc23aa10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shapefile_by_intersection(gpd_data,\n",
    "                                     gpd_filter,\n",
    "                                     filtertype=\"intersects\",\n",
    "                                     invert_mask=True,\n",
    "                                     return_inverse=False):\n",
    "    \"\"\"\n",
    "    Filter out polygons that intersect with another polygon shapefile. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gpd_data: geopandas GeoDataFrame\n",
    "        Polygon data to be filtered.\n",
    "    gpd_filter: geopandas GeoDataFrame\n",
    "        Polygon dataset to be used as a filter.\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    filtertype: default = 'intersects'\n",
    "        Options = ['intersects', 'contains', 'within']\n",
    "    invert_mask: boolean\n",
    "        Default = 'True'. This determines whether you want areas that \n",
    "        DO ( = 'False') or DON'T ( = 'True') intersect with the filter dataset.\n",
    "    return_inverse: boolean\n",
    "        Default = 'False'. If True, then return both parts of the intersection:\n",
    "        - those that intersect AND \n",
    "        - those that don't as two GeoDataFrames.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gpd_data_filtered: geopandas GeoDataFrame\n",
    "        If invert_mask==True, `gpd_data_filtered` is a filtered polygon set, \n",
    "        with polygons that DO intersect with gpd_filter removed. \n",
    "        If invert_mask==False, `gpd_data_filtered` is a filtered polygon set, \n",
    "        with polygons that DON'T intersect with gpd_filter removed. \n",
    "    intersect_index: list of indices of gpd_data that intersect with gpd_filter.\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    if 'return_inverse = True'\n",
    "    gpd_data_inverse: geopandas GeoDataFrame\n",
    "        If invert_mask==True, `gpd_data_inverse` is a filtered polygon set, \n",
    "        with polygons that DON'T intersect with gpd_filter removed (inverse of gpd_data_filtered).\n",
    "        If invert_mask==False, `gpd_data_inverse` is a filtered polygon set, \n",
    "        with polygons that DO intersect with gpd_filter removed (inverse of gpd_data_filtered). \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the coordinate reference systems of both GeoDataFrames are the same.\n",
    "    assert gpd_data.crs == gpd_filter.crs\n",
    "\n",
    "    # Find the index of all the polygons in gpd_data that intersect with gpd_filter.\n",
    "    intersections = gpd_filter.sjoin(gpd_data,\n",
    "                                     how=\"inner\",\n",
    "                                     predicate=filtertype)\n",
    "    intersect_index = np.sort(intersections[\"index_right\"].unique())\n",
    "\n",
    "    if invert_mask:\n",
    "        # Grab only the polygons that are NOT in the intersect_index.\n",
    "        gpd_data_filtered = gpd_data.loc[~gpd_data.index.isin(intersect_index)]\n",
    "    else:\n",
    "        # Grab only the polygons that ARE in the intersect_index.\n",
    "        gpd_data_filtered = gpd_data.loc[gpd_data.index.isin(intersect_index)]\n",
    "\n",
    "    if return_inverse:\n",
    "        # We need to use the indices from intersect_index to find the inverse dataset, so we\n",
    "        # will just swap the '~'.\n",
    "\n",
    "        if invert_mask:\n",
    "            # Grab only the polygons that ARE in the intersect_index.\n",
    "            gpd_data_inverse = gpd_data.loc[gpd_data.index.isin(\n",
    "                intersect_index)]\n",
    "        else:\n",
    "            # Grab only the polygons that are NOT in the intersect_index.\n",
    "            gpd_data_inverse = gpd_data.loc[~gpd_data.index.isin(intersect_index\n",
    "                                                                )]\n",
    "\n",
    "        return gpd_data_filtered, intersect_index, gpd_data_inverse\n",
    "    else:\n",
    "        return gpd_data_filtered, intersect_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c810e54-9ef6-4f78-9451-08afabd6df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_driver(file_path):\n",
    "    \"\"\"\n",
    "    Function to optain the fiona driver to use based \n",
    "    on the file extension of a file path provided.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    file_path: str \n",
    "        File path string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    file_driver: str\n",
    "        Fiona driver to use for the file path\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    file_extension = os.path.splitext(file_path)[-1]\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == \".geojson\":\n",
    "        file_driver = \"GeoJSON\"\n",
    "    elif file_extension == \".shp\":\n",
    "        file_driver = \"ESRI Shapefile\"\n",
    "\n",
    "    return file_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8896f9b-e845-4b65-b9e8-ad6bf2db0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the urban mask polygon dataset from the High Resolution Population Density Maps dataset.\n",
    "# This dataset estimates the number of people living within 30-meter grid tiles in nearly every country around the world.\n",
    "# https://registry.opendata.aws/dataforgood-fb-hrsl/\n",
    "\n",
    "\n",
    "def generate_urban_mask_from_hrpdm(tiles, high_population_density_threshold,\n",
    "                                   output_urban_mask_fp, dask_chunks):\n",
    "    \"\"\"\n",
    "    Generate polygons for high density population areas from the High \n",
    "    Resolution Population Density Maps dataset based on the \n",
    "    `high_population_density_threshold` threshold specified. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    tiles: geopandas GeoDataFrame\n",
    "        Tiles covering the area of interest. \n",
    "    dask_chunks: dict\n",
    "        Chunk sizes along each dimension to use to coerce the \n",
    "        High Resolution Population Density Maps array's data \n",
    "        into a dask array. \n",
    "    high_population_density_threshold: float\n",
    "        Threshold to use to generate a mask to obtain high population \n",
    "        density areas from the High Resolution Population Density Maps data.\n",
    "    output_urban_mask_fp: str\n",
    "        File path to write to disk the high population density polygons generated.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    output_urban_mask_fp: str\n",
    "        File path to where the high population density polygons have been written to disk. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    original_tiles_crs = tiles.crs\n",
    "\n",
    "    output_file_driver = get_file_driver(output_urban_mask_fp)\n",
    "\n",
    "    # Set s3 region for HRSL data access.\n",
    "    os.environ['AWS_DEFAULT_REGION'] = \"us-east-1\"\n",
    "    os.environ['AWS_S3_ENDPOINT'] = \"s3.us-east-1.amazonaws.com\"\n",
    "\n",
    "    # Load the High Resolution Population Density Maps data.\n",
    "    overall_population_density_fp = \"s3://dataforgood-fb-data/hrsl-cogs/hrsl_general/hrsl_general-latest.vrt\"\n",
    "    overall_population_density = rioxarray.open_rasterio(\n",
    "        overall_population_density_fp, chunks=dask_chunks).squeeze()\n",
    "\n",
    "    # Use the defined threshold to filter the dataset to get the high population density areas.\n",
    "    high_population_density = overall_population_density > high_population_density_threshold\n",
    "\n",
    "    # Transform the tiles into the same CRS as the High Resolution Population Density Maps data.\n",
    "    new_tiles_crs = overall_population_density.rio.crs\n",
    "    tiles_transformed = tiles.to_crs(new_tiles_crs)\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for tile in tiles_transformed.itertuples():\n",
    "        tile_id = tile.region_code\n",
    "        tile_geometry = tile.geometry\n",
    "        \n",
    "        minx, miny, maxx, maxy = tile_geometry.bounds\n",
    "\n",
    "        # Subset the overall_population_density using the tile's geometry bounds.\n",
    "        tile_high_population_density = high_population_density.sel(\n",
    "            x=slice(minx, maxx), y=slice(maxy, miny))\n",
    "\n",
    "        tile_high_population_density_polygons = xr_vectorize(\n",
    "            tile_high_population_density,\n",
    "            mask=(tile_high_population_density == True),\n",
    "            crs=tile_high_population_density.geobox.crs,\n",
    "            transform=tile_high_population_density.geobox.transform)\n",
    "\n",
    "        tile_high_population_density_polygons = tile_high_population_density_polygons.to_crs(\n",
    "            original_tiles_crs)\n",
    "\n",
    "        if len(tile_high_population_density_polygons) >= 1:\n",
    "\n",
    "            # Combine the polygons into a multi-polygon.\n",
    "            merged_tile_high_population_density_polygons_geoms = shapely.ops.unary_union(\n",
    "                tile_high_population_density_polygons['geometry'])\n",
    "\n",
    "            # Turn the combined multipolygon back into a GeoDataFrame.\n",
    "            merged_tile_high_population_density_polygons = gpd.GeoDataFrame(\n",
    "                geometry=list(\n",
    "                    merged_tile_high_population_density_polygons_geoms.geoms))\n",
    "\n",
    "            # We need to add the crs back onto the GeoDataFrame.\n",
    "            merged_tile_high_population_density_polygons.crs = original_tiles_crs\n",
    "\n",
    "            # Save the polygons to a shapefile.\n",
    "            schema = {\"geometry\": \"Polygon\"}\n",
    "\n",
    "            if os.path.isfile(output_urban_mask_fp):\n",
    "                with fiona.open(output_urban_mask_fp,\n",
    "                                mode=\"w\",\n",
    "                                crs=original_tiles_crs,\n",
    "                                driver=output_file_driver,\n",
    "                                schema=schema) as output:\n",
    "                    for row in merged_tile_high_population_density_polygons.itertuples(\n",
    "                    ):\n",
    "                        # Make a dictionary from the shapely Polygon object.\n",
    "                        row_geom = shapely.geometry.mapping(row.geometry)\n",
    "                        output.write(({'geometry': row_geom}))\n",
    "\n",
    "            else:\n",
    "                with fiona.open(output_urban_mask_fp,\n",
    "                                mode=\"w\",\n",
    "                                crs=original_tiles_crs,\n",
    "                                driver=output_file_driver,\n",
    "                                schema=schema) as output:\n",
    "                    for row in merged_tile_high_population_density_polygons.itertuples(\n",
    "                    ):\n",
    "                        # Make a dictionary from the shapely Polygon object.\n",
    "                        row_geom = shapely.geometry.mapping(row.geometry)\n",
    "                        output.write(({'geometry': row_geom}))\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f'\\nTile {tile_id} did not run. \\n'\n",
    "                f'This is probably because there are no high population density polygons present in this tile.'\n",
    "            )\n",
    "\n",
    "    # Set s3 region back for DE Africa data access.\n",
    "    os.environ['AWS_DEFAULT_REGION'] = \"af-south-1\"\n",
    "    os.environ['AWS_S3_ENDPOINT'] = \"s3.af-south-1.amazonaws.com\"\n",
    "\n",
    "    if counter >= 1:\n",
    "        print(\n",
    "            f\"\\nThe high population density polygons have been written to: {output_urban_mask_fp}\"\n",
    "        )\n",
    "        return output_urban_mask_fp\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nNo high population density polygons have been found for any of the tiles.\"\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae5375-e7e3-48a7-abbf-963d0a95edf9",
   "metadata": {},
   "source": [
    "## Define Analysis Parameters\n",
    "\n",
    "The following section walks you through the analysis parameters you will need to set for this workflow. Each section describes the parameter, how it is used, and what value was used for the DE Africa Waterbodies product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289de1c-2f87-4c6b-8316-966c5c382946",
   "metadata": {},
   "source": [
    "### Set up some file names for the inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef421b-cec1-4433-8d33-d53e2461054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the outputs.\n",
    "output_dir = \"waterbodies_outputs\"\n",
    "output_dir_fp = Path(output_dir)\n",
    "os.makedirs(output_dir_fp, exist_ok=True)\n",
    "\n",
    "# Set up some filenames to use.\n",
    "base_filename = \"SenegalBasinWaterbodies\"\n",
    "base_filename_fp = output_dir_fp / base_filename\n",
    "              \n",
    "os.makedirs(base_filename_fp, exist_ok=True)\n",
    "\n",
    "# Putting this here to allow for testing of both methods\n",
    "# to handle large polygons. \n",
    "#handle_large_polygons = 'erode-dilate-v1'\n",
    "#handle_large_polygons = 'erode-dilate-v2'\n",
    "handle_large_polygons = 'nothing'\n",
    "\n",
    "# The name and filepath of the first temporary polygon dataset.\n",
    "waterbodies_shapefile_temp = base_filename_fp / f\"temp_{handle_large_polygons.replace('-','_')}\"\n",
    "# The filepath for the location of temp files during the code run.\n",
    "waterbodies_shapefile_merged = base_filename_fp / f\"merged_{handle_large_polygons.replace('-','_')}\"\n",
    "# The name and filepath of the outputs following the filtering steps\n",
    "waterbodies_shapefile_filtered = base_filename_fp / f\"filtered_{handle_large_polygons.replace('-','_')}\"\n",
    "# The name and file path of the final, completed waterbodies shapefile\n",
    "waterbodies_shapefile_final = base_filename_fp / f\"SenegalBasinWaterbodies_{handle_large_polygons.replace('-','_')}\" #\"AfricaWaterbBodies\"\n",
    "\n",
    "# File extension to use for outputs , either .shp (ESRI Shapefile) or .geojson (GeoJSON)\n",
    "file_extension = \".geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20236caf-b0eb-4e1e-8c1c-c6a318f5edb3",
   "metadata": {},
   "source": [
    "### Define parameters to use when loading data from the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4bd317-4be1-4fd1-939b-9b8f1d4fd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_chunks = {'x': 3000, 'y': 3000, 'time': 1}\n",
    "# Resolution of the WOfS datasets.\n",
    "resolution = (-30, 30)\n",
    "# CRS to work with for all files.\n",
    "crs = \"EPSG:6933\"\n",
    "\n",
    "# Create a datacube query.\n",
    "query = dict(dask_chunks=dask_chunks, resolution=resolution, output_crs=crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64866886-3224-452e-b07e-966ac709cd04",
   "metadata": {},
   "source": [
    "### Set the analysis region\n",
    "If you would like to perform the analysis for all of Africa, using the published WOfS All-time Summary, set `all_of_africa = True`. If you set the flag `all_of_africa` to `False`, you will need to provide either a latitude and longitude range covering the area of interest, a path to the shapefile / GeoJSON defining the area of interest, or a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294ec5c-53f3-455a-b650-975149682a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_of_africa = False\n",
    "\n",
    "if not all_of_africa:\n",
    "    #\"\"\"\n",
    "    # Load a shapefile or GeoJSON file for the area of interest:\n",
    "    #basin = define_area(shapefile_path=\"data/SenegalBasin.geojson\")\n",
    "    #geopolygon = geometry.Geometry(basin.features[0][\"geometry\"], crs=\"epsg:4326\")\n",
    "    #\"\"\"\n",
    "    \"\"\"\n",
    "    # Use the section below if you would like to define the area of interest using a bounding box.\n",
    "    # Bounding box for a section of Bukama in the Democratic Republic of Congo\n",
    "    # that covers the lakes Kabwe, Kabele and Mulenda.\n",
    "    bbox = (25.752395, -9.267306, 26.189124, -8.610346)\n",
    "    left, bottom, right, top = bbox\n",
    "    geopolygon = geometry.box(left, bottom, right, top, crs=\"EPSG:4326\")\n",
    "    \"\"\"\n",
    "elif all_of_africa:\n",
    "    geopolygon = None\n",
    "\n",
    "# Generate the tiles to be used in this workflow.\n",
    "tiles = get_tiles(aoi_geopolygon=geopolygon,\n",
    "                  all_of_africa=all_of_africa,\n",
    "                  crs=crs)\n",
    "\n",
    "# Uncomment the section below to output the tiles.\n",
    "#tiles_output_fp = base_filename_fp/f\"tiles{file_extension}\"\n",
    "#tiles.to_file(tiles_output_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2c027-1c6f-4fda-9343-b3d3a78328b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='wetnessthreshold'></a>\n",
    "### How frequently wet does a pixel need to be to be included?\n",
    "The value/s set here will be the minimum frequency (as a decimal between 0 and 1) that you want water to be detected across all analysis years before it is included. \n",
    "\n",
    "E.g. If this was set to 0.10, any pixels that are wet *at least* 10% of the time across all valid observations will be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "Following the exploration of an appropriate wetness threshold for DE Africa Waterbodies [see here]( Add-link-to-notebook-showing-threshold-sensisitivity-analysis), we choose to set two thresholds here. The code is set up to loop through both wetness thresholds, and to write out two temporary shapefiles. These two shapefiles with two separate thresholds are then used together to combine polygons from both thresholds later on in the workflow.\n",
    "\n",
    "Polygons identified by the secondary threshold that intersect with the polygons generated by the primary threshold will be extracted, and included in the final polygon dataset. This means that the **location** of polygons is set by the primary threshold, but the **shape** of these polygons is set by the secondary threshold.\n",
    "\n",
    "Threshold values need to be provided as a list of either one or two floating point numbers. If one number is provided, then this will be used to generate the initial polygon dataset. If two thresholds are entered, the **first number becomes the secondary threshold, and the second number becomes the primary threshold**. If more than two numbers are entered, the code will generate an error below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81f9ff-4000-47d0-977c-7a15af0514c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the smaller threshold is first in the list if using 2 thresholds.\n",
    "secondary_threshold = 0.05\n",
    "primary_threshold = 0.1\n",
    "minimum_wet_thresholds = [secondary_threshold, primary_threshold]\n",
    "\n",
    "# Test whether the wetness threshold has been correctly set.\n",
    "if len(minimum_wet_thresholds) == 2:\n",
    "    print(\n",
    "        f'We will be running a hybrid wetness threshold. Please ensure that the major/primary threshold is \\n'\n",
    "        f'listed second, with the supplementary threshold entered first.  \\n'\n",
    "        f'**You have set {minimum_wet_thresholds[-1]} as the primary threshold, which will define the location of the waterbodies polygons** \\n'\n",
    "        f'**with {minimum_wet_thresholds[0]} set as the supplementary threshold, this will define the extent / shape of the waterbodies polygons**'\n",
    "    )\n",
    "elif len(minimum_wet_thresholds) == 1:\n",
    "    print(\n",
    "        f'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "        f'set this option by including two wetness thresholds in the `minimum_wet_thresholds` variable above. \\n'\n",
    "        f'The wetness threshold we will use is {minimum_wet_thresholds}.')\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f'There is something wrong with your entered wetness threshold. Please enter a list \\n'\n",
    "        f'of either one or two numbers. You have entered {minimum_wet_thresholds}. \\n'\n",
    "        f'See above for more information')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c5ac3-d193-416f-9839-936dad86f806",
   "metadata": {},
   "source": [
    "<a id='size'></a>\n",
    "\n",
    "### How big/small should the polygons be?\n",
    "This filtering step can remove very small and/or very large waterbody polygons. The size listed here is in m<sup>2</sup>. A single pixel in Landsat data is 30 m x 30 m = 900 m<sup>2</sup>. \n",
    "\n",
    "**MinSize**\n",
    "\n",
    "E.g. A minimum size of 9000 m<sup>2</sup> means that polygons need to be at least 10 pixels to be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "**MaxSize**\n",
    "\n",
    "E.g. A maximum size of 1 000 000 m<sup>2</sup> means that you only want to consider polygons less than 1 km<sup>2</sup>. If you don't want to use this filter, set this number to `math.inf`. \n",
    "\n",
    "*NOTE: if you are doing this analysis for all of Africa, very large polygons will be generated offshore, in the steps prior to filtering by the specified `land_sea_mask`. For this reason, we have used a `max_polygon_size` = Area of Lake Victoria (the largest lake in Africa). This will remove the huge ocean polygons, but keep large inland waterbodies that we want to map.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206fa47-6d55-43f3-a7af-98f98b28ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_polygon_size = 4500  # 5 pixels\n",
    "max_polygon_size = math.inf  #59947000000 approx area of Lake Victoria 59947 sq. km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a9d28-8b7e-4453-a20a-b873455ffdb6",
   "metadata": {},
   "source": [
    "### Filter results based on number of valid observations\n",
    "\n",
    "The total number of valid WOfS observations for each pixel varies depending on the frequency of clouds and cloud shadow, the proximity to high slope and terrain shadow, and the seasonal change in solar angle. \n",
    "\n",
    "The `count_clear` parameter within the [`wofs_ls_summary_alltime`](https://explorer.digitalearth.africa/products/wofs_ls_summary_alltime) data provides a count of the number of valid observations each pixel recorded over the analysis period. We can use this parameter to mask out pixels that were infrequently observed. \n",
    "If this mask is not applied, pixels that were observed only once could be included if that observation was wet (i.e. a single wet observation means the calculation of the frequency statistic would be (1 wet observation) / (1 total observation) = 100% frequency of wet observations).\n",
    "\n",
    "Here we set the minimum number of observations to be 128 (roughly 4 per year over our 32 year analysis). Note that this parameter does not specify the timing of these observations, but rather just the **total number of valid observations** (observed at any time of the year, in any year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e21ec-c9a1-4450-bcf9-999f521ddd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year = datetime.datetime.now().year\n",
    "start_year_wofs_dataset = 1984\n",
    "min_valid_observations_yearly = 4\n",
    "\n",
    "no_of_years = this_year - start_year_wofs_dataset\n",
    "\n",
    "#min_valid_observations = min_valid_observations_yearly * no_of_years\n",
    "\n",
    "min_valid_observations = 128\n",
    "\n",
    "print(min_valid_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e49a5a-978b-466b-84b8-31578121b043",
   "metadata": {},
   "source": [
    "<a id='coastline'></a>\n",
    "### Read in a land/sea mask\n",
    "\n",
    "You can choose which land/sea mask you would like to use to mask out ocean polygons, depending on how much coastal water you would like in the final product. \n",
    "\n",
    "We use the [Marine Regions Global Oceans and Seas v01 dataset](https://www.marineregions.org/sources.php#goas). Any polygons that intersect with this mask are filtered out, i.e. if a polygon identified within our workflow overlaps with this coastal mask by even a single pixel, it will be discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eae51e-87bf-483d-82cb-a4da43f4ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_sea_mask_fp = \"data/GOaS_v1_20211214/goas_v01.shp\"\n",
    "#land_sea_mask_fp = \"data/water-polygons-split-4326/water_polygons.shp\"\n",
    "land_sea_mask = gpd.read_file(land_sea_mask_fp).to_crs(crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db9fdc-fea1-45e9-b8e6-4f9153151cdb",
   "metadata": {},
   "source": [
    "<a id='rivers'></a>\n",
    "### Do you want to filter out polygons that intersect with major rivers?\n",
    "\n",
    "This filtering step is done to remove river segments from the polygon dataset. \n",
    "Set the filepath to the dataset you would wish to use in the `major_rivers_fp` variable. The dataset needs to be a vector dataset, and [able to be read in by the fiona python library](https://fiona.readthedocs.io/en/latest/fiona.html#fiona.open).\n",
    "\n",
    "Note that we reproject this dataset to the CRS specified in the variable `crs` to match the coordinate reference system of the WOfS data we use. A list of epsg code [can be found here](https://spatialreference.org/ref/epsg/).\n",
    "\n",
    "If you don't want to filter out polygons that intersect with rivers, set this parameter to `False`.\n",
    "\n",
    "**Note that for the DE Africa Water Body Polygon dataset, we set this filter to False (`filter_out_rivers = False`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd024b4-270f-4c8e-92ab-7ba52a46b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out_rivers = False\n",
    "\n",
    "if filter_out_rivers:\n",
    "    # Insert path to the dataset location.\n",
    "    major_rivers_fp = \"\"\n",
    "    major_rivers = gpd.GeoDataFrame.from_file(major_rivers_fp)\n",
    "    major_rivers = major_rivers.to_crs(crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bd06f-cd83-4e53-a1bc-c213fd496206",
   "metadata": {},
   "source": [
    "<a id='Urban'></a>\n",
    "\n",
    "### Read in a mask for high-rise CBDs\n",
    "\n",
    "WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This results in 'waterbodies' around these misclassified shadows in capital cities. \n",
    "\n",
    "To address this problem, we use the High Resolution Population Density Maps dataset to define a spatial footprint for Africa's CBD areas. The theory of using this dataset is that high-rises have a high population density (population count per area). Therefore pixels in the HRPDM dataset with a higher general population density than the specified threshold are vectorized and used as our CBD filter.\n",
    "\n",
    "If you are not using WOfS for your analysis, you may choose to set `filter_out_urban_areas = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340d819-aa2e-46b3-b35c-08596477c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out_urban_areas = False #True\n",
    "\n",
    "if filter_out_urban_areas:\n",
    "    # You can load a shapefile or GeoJSON file from disk to use to filter out\n",
    "    # high-rise CBDs.\n",
    "    # urban_mask_fp = \"\"\n",
    "    # urban_mask = gpd.read_file(urban_mask_fp).to_crs(crs)\n",
    "\n",
    "    # Or you can generate the urban mask polygon dataset\n",
    "    # from the High Resolution Population Density Maps dataset.\n",
    "    output_fp = base_filename_fp / f'hrpdm_high_popolation_density_polygons{file_extension}'\n",
    "\n",
    "    chunks_xy = {\"x\": dask_chunks[\"x\"], \"y\": dask_chunks[\"y\"]}\n",
    "\n",
    "    urban_mask_fp = generate_urban_mask_from_hrpdm(\n",
    "        tiles,\n",
    "        high_population_density_threshold=10,\n",
    "        output_urban_mask_fp=output_fp,\n",
    "        dask_chunks=chunks_xy)\n",
    "\n",
    "    if urban_mask_fp is not None:\n",
    "        urban_mask = gpd.read_file(urban_mask_fp).to_crs(crs)\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo high population density polygons found. \\\n",
    "            \\n`urban_mask` set to None. \\\n",
    "            \\nPlease use a lower threshold.\")\n",
    "        urban_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb76c9-9c79-4a0d-a2a6-f424f26fcf16",
   "metadata": {},
   "source": [
    "## Generate the first temporary polygon dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439148e2-bff1-44e3-9397-05d921b5cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with datacube.Datacube(app='WaterbodiesPolygons') as dc:\n",
    "\n",
    "    for tile in tiles.itertuples():\n",
    "\n",
    "        tile_id = tile.region_code\n",
    "        tile_geopolygon = geometry.Geometry(geom=tile.geometry, crs=tiles.crs)\n",
    "\n",
    "        # Generate waterbodies polygons for each tile.\n",
    "        try:\n",
    "            # Update the datacube query with the tile geometry.\n",
    "            query.update(geopolygon=tile_geopolygon)\n",
    "\n",
    "            # Load the WOfS All-Time Summary of clear and wet observations.\n",
    "            wofs_alltime_summary = dc.load('wofs_ls_summary_alltime',\n",
    "                                           **query).squeeze()\n",
    "\n",
    "            # Set the no-data values to nan.\n",
    "            # Masking here is done using the frequency measurement because for multiple\n",
    "            # areas NaN values are present in the frequency measurement but the\n",
    "            # no data value -999 is not present in the count_clear and\n",
    "            # count_wet measurements.\n",
    "            # Note: it seems some pixels with NaN values in the frequency measurement\n",
    "            # have a value of zero in the count_clear and/or the count_wet measurements.\n",
    "            wofs_alltime_summary = wofs_alltime_summary.where(\n",
    "                ~np.isnan(wofs_alltime_summary.frequency))\n",
    "\n",
    "            # Mask pixels not observed at least min_valid_observations times.\n",
    "            wofs_alltime_summary_valid_clear_count = wofs_alltime_summary.count_clear >= min_valid_observations\n",
    "\n",
    "            for threshold in minimum_wet_thresholds:\n",
    "                # Mask any pixels whose frequency of water detection is less than the threshold.\n",
    "                wofs_alltime_summary_valid_wetness = wofs_alltime_summary.frequency > threshold\n",
    "\n",
    "                # Now find pixels that meet both the minimum valid observations\n",
    "                # and minimum wet threshold criteria.\n",
    "                wofs_alltime_summary_valid = wofs_alltime_summary_valid_wetness.where(\n",
    "                    wofs_alltime_summary_valid_wetness &\n",
    "                    wofs_alltime_summary_valid_clear_count)\n",
    "\n",
    "                # Convert the raster to polygons.\n",
    "                # We use a mask of '1' to only generate polygons around values of '1' (not NaNs).\n",
    "                polygons_mask = wofs_alltime_summary_valid == 1\n",
    "\n",
    "                polygons = xr_vectorize(\n",
    "                    wofs_alltime_summary_valid,\n",
    "                    mask=polygons_mask,\n",
    "                    crs=crs,\n",
    "                    transform=wofs_alltime_summary.geobox.transform)\n",
    "                #polygons = polygons[polygons.attribute == 1].reset_index(drop=True)\n",
    "\n",
    "                # Combine any overlapping polygons.\n",
    "                merged_polygon_geoms = shapely.ops.unary_union(\n",
    "                    polygons['geometry'])\n",
    "\n",
    "                # Turn the combined multipolygon back into a GeoDataFrame.\n",
    "                merged_polygons = gpd.GeoDataFrame(\n",
    "                    geometry=list(merged_polygon_geoms.geoms))\n",
    "                # We need to add the crs back onto the GeoDataFrame.\n",
    "                merged_polygons.crs = crs\n",
    "\n",
    "                # Calculate the area of each polygon again now that overlapping\n",
    "                # polygons have been merged.\n",
    "                merged_polygons['area'] = merged_polygons.area\n",
    "\n",
    "                # Write the polygons to disk.\n",
    "                schema = {\n",
    "                    \"geometry\": \"Polygon\",\n",
    "                    \"properties\": {\n",
    "                        \"area\": \"float\",\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                output_fp = f'{waterbodies_shapefile_temp}_{threshold}{file_extension}'\n",
    "\n",
    "                if os.path.isfile(output_fp):\n",
    "                    with fiona.open(output_fp,\n",
    "                                    mode=\"a\",\n",
    "                                    crs=crs,\n",
    "                                    driver=get_file_driver(output_fp),\n",
    "                                    schema=schema) as output:\n",
    "                        for row in merged_polygons.itertuples():\n",
    "                            row_area = row.area\n",
    "                            # Make a dictionary from the shapely Polygon object.\n",
    "                            row_geom = shapely.geometry.mapping(row.geometry)\n",
    "\n",
    "                            output.write(({\n",
    "                                'properties': {\n",
    "                                    'area': row_area,\n",
    "                                },\n",
    "                                'geometry': row_geom\n",
    "                            }))\n",
    "\n",
    "                else:\n",
    "                    with fiona.open(output_fp,\n",
    "                                    mode=\"w\",\n",
    "                                    crs=crs,\n",
    "                                    driver=get_file_driver(output_fp),\n",
    "                                    schema=schema) as output:\n",
    "                        for row in merged_polygons.itertuples():\n",
    "                            row_area = row.area\n",
    "                            # Make a dictionary from the shapely Polygon object.\n",
    "                            row_geom = shapely.geometry.mapping(row.geometry)\n",
    "\n",
    "                            output.write(({\n",
    "                                'properties': {\n",
    "                                    'area': row_area,\n",
    "                                },\n",
    "                                'geometry': row_geom\n",
    "                            }))\n",
    "        except:\n",
    "            print(\n",
    "                f'\\nTile {tile_id} did not run. \\n'\n",
    "                f'This is probably because there are no waterbodies present in this tile.'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323aefd0-5e66-4808-8ab0-580c57c26bff",
   "metadata": {},
   "source": [
    "## Merge polygons that have an edge at a tile boundary\n",
    "\n",
    "Now that we have all of the polygons across our whole region of interest, we need to check for artifacts in the data caused by tile boundaries.\n",
    "\n",
    "We have created a GeoDataFrame `buffered_30m_tiles`, that consists of the tile boundaries, plus a 1 pixel (30 m) buffer. This GeoDataFrame will help us to find any polygons that have a boundary at the edge of a tile. We can then find where polygons touch across this boundary, and join them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d096050-0674-437c-b15a-d8946d8be760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 1 pixel (30 m) buffer to the tiles boundary.\n",
    "buffered_30m_tiles = gpd.GeoDataFrame.copy(tiles, deep=True)\n",
    "buffered_30m_tiles[\"geometry\"] = buffered_30m_tiles.boundary.buffer(\n",
    "    30, cap_style=\"flat\", join_style=\"mitre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae423171-7fab-4128-8537-f590e44cd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in minimum_wet_thresholds:\n",
    "    print(f'Working on {threshold} polygons')\n",
    "\n",
    "    # We are using the more severe wetness threshold as the main polygon dataset.\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'minimum_wet_thresholds'\n",
    "    # variable, with the higher threshold listed second.\n",
    "    waterbodies_polygons = gpd.read_file(\n",
    "        f'{waterbodies_shapefile_temp}_{threshold}{file_extension}')\n",
    "\n",
    "    boundary_polygons, intersect_indices, not_boundary_polygons = filter_shapefile_by_intersection(\n",
    "        waterbodies_polygons,\n",
    "        buffered_30m_tiles,\n",
    "        invert_mask=False,\n",
    "        return_inverse=True)\n",
    "\n",
    "    # Now combine overlapping polygons in boundary_polygons.\n",
    "    merged_boundary_polygons_geoms = shapely.ops.unary_union(\n",
    "        boundary_polygons['geometry'])\n",
    "\n",
    "    # `Explode` the multipolygon back out into individual polygons.\n",
    "    merged_boundary_polygons = gpd.GeoDataFrame(\n",
    "        crs=waterbodies_polygons.crs, geometry=[merged_boundary_polygons_geoms])\n",
    "    merged_boundary_polygons = merged_boundary_polygons.explode(\n",
    "        index_parts=True)\n",
    "\n",
    "    # Then combine our merged_boundary_polygons with the not_boundary_polygons.\n",
    "    all_polygons = gpd.GeoDataFrame(\n",
    "        pd.concat([not_boundary_polygons, merged_boundary_polygons],\n",
    "                  ignore_index=True,\n",
    "                  sort=True)).set_geometry('geometry')\n",
    "\n",
    "    # Calculate the area of each polygon in the merged polygon set.\n",
    "    all_polygons[\"area\"] = all_polygons.area\n",
    "\n",
    "    # Check for NaNs\n",
    "    all_polygons.dropna(inplace=True)\n",
    "\n",
    "    # Export the all_polygons to disk.\n",
    "    output_fp = f'{waterbodies_shapefile_merged}_{threshold}{file_extension}'\n",
    "\n",
    "    schema = {\"geometry\": \"Polygon\", \"properties\": {\"area\": \"float\",}}\n",
    "\n",
    "    with fiona.open(output_fp,\n",
    "                    mode=\"w\",\n",
    "                    crs=crs,\n",
    "                    driver=get_file_driver(output_fp),\n",
    "                    schema=schema) as output:\n",
    "        for row in all_polygons.itertuples():\n",
    "            row_area = row.area\n",
    "            # Make a dictionary from the shapely Polygon object.\n",
    "            row_geom = shapely.geometry.mapping(row.geometry)\n",
    "\n",
    "            output.write(({\n",
    "                'properties': {\n",
    "                    'area': row_area,\n",
    "                },\n",
    "                'geometry': row_geom\n",
    "            }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20febf05-883e-4678-b75e-60aecf3666e5",
   "metadata": {},
   "source": [
    "<a id='Filtering'></a>\n",
    "\n",
    "## Filter the merged polygons by:\n",
    "- **Area:**\n",
    "Based on the `min_polygon_size` and `max_polygon_size` parameters set [here](#size).\n",
    "- **Coastline:**\n",
    "Using the `land_sea_mask` dataset loaded [here](#coastline).\n",
    "- **CBD location (optional):**\n",
    "Using the `urban_mask` dataset loaded [here](#Urban).\n",
    "- **Wetness thresholds:**\n",
    "Here we apply the hybrid threshold described [here](#wetnessthreshold)\n",
    "- **Intersection with rivers (optional):**\n",
    "Using the `major_rivers` dataset loaded [here](#rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b391a-88ae-4772-af09-eac610ef14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_threshold_polygons = gpd.read_file(\n",
    "    f'{waterbodies_shapefile_merged}_{minimum_wet_thresholds[-1]}{file_extension}'\n",
    ")\n",
    "\n",
    "primary_threshold_polygons['area'] = pd.to_numeric(\n",
    "    primary_threshold_polygons.area)\n",
    "\n",
    "# Filter out any polygons smaller than min_polygon_size, and greater than max_polygon_size.\n",
    "area_filtered_primary_threshold_polygons = primary_threshold_polygons.loc[(\n",
    "    (primary_threshold_polygons['area'] > min_polygon_size) &\n",
    "    (primary_threshold_polygons['area'] <= max_polygon_size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e824714-6dfe-4830-bec8-25cb3934646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered out any ocean polygons using the land/sea mask.\n",
    "inland_primary_threshold_polygons, intersect_indices = filter_shapefile_by_intersection(\n",
    "    area_filtered_primary_threshold_polygons, land_sea_mask, invert_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594182ed-77fd-40af-8348-350018f1f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was set to false in order to generate the urban mask polygon but not use it in masking.\n",
    "filter_out_urban_areas = False\n",
    "\n",
    "if filter_out_urban_areas and urban_mask is not None:\n",
    "    cbd_filtered_primary_threshold_polygons, intersect_indices = filter_shapefile_by_intersection(\n",
    "        inland_primary_threshold_polygons, urban_mask)\n",
    "else:\n",
    "    print(\n",
    "        'You have chosen not to filter out waterbodies within CBDs. If you meant to use this option, please \\n'\n",
    "        'set `filter_out_urban_areas = True` variable above, and set the path to your urban filter shapefile'\n",
    "    )\n",
    "    cbd_filtered_primary_threshold_polygons = inland_primary_threshold_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f181a-e18e-4a20-9a17-60a5782326fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for hybrid wetness thresholds.\n",
    "if len(minimum_wet_thresholds) == 2:\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'minimum_wet_thresholds'\n",
    "    # variable, with the supplementary threshold listed first.\n",
    "    secondary_threshold_polygons = gpd.read_file(\n",
    "        f'{waterbodies_shapefile_merged}_{minimum_wet_thresholds[0]}{file_extension}'\n",
    "    )\n",
    "    secondary_threshold_polygons['area'] = pd.to_numeric(\n",
    "        secondary_threshold_polygons.area)\n",
    "\n",
    "    # Filter out any polygons greater than max_polygon_size.\n",
    "    area_filtered_secondary_threshold_polygons = secondary_threshold_polygons.loc[\n",
    "        (secondary_threshold_polygons['area'] <= max_polygon_size)]\n",
    "    \n",
    "    # Filtered out any ocean polygons using the land/sea mask.\n",
    "    inland_secondary_threshold_polygons, intersect_indices = filter_shapefile_by_intersection(\n",
    "        area_filtered_secondary_threshold_polygons, land_sea_mask, invert_mask=True)\n",
    "    \n",
    "    # Find the polygons identified using the secondary threshold that intersect with those identified\n",
    "    # using the primary threshold.\n",
    "    dont_intersect_with_primary_threshold_polygons, intersect_indices = filter_shapefile_by_intersection(\n",
    "        inland_secondary_threshold_polygons,\n",
    "        cbd_filtered_primary_threshold_polygons)\n",
    "\n",
    "    do_intersect_with_primary_threshold_polygons = inland_secondary_threshold_polygons.loc[\n",
    "        inland_secondary_threshold_polygons.index.isin(\n",
    "            intersect_indices)]\n",
    "\n",
    "    # Concat the two polygon datasets together.\n",
    "    combined_polygons = gpd.GeoDataFrame(\n",
    "        pd.concat([\n",
    "            do_intersect_with_primary_threshold_polygons,\n",
    "            cbd_filtered_primary_threshold_polygons\n",
    "        ],\n",
    "                  ignore_index=True))\n",
    "    # Merge overlapping polygons.\n",
    "    merged_combined_polygons_geoms = combined_polygons.unary_union\n",
    "    # `Explode` the multipolygon back out into individual polygons.\n",
    "    merged_combined_polygons = gpd.GeoDataFrame(\n",
    "        crs=secondary_threshold_polygons.crs,\n",
    "        geometry=[merged_combined_polygons_geoms])\n",
    "    merged_combined_polygons = merged_combined_polygons.explode(\n",
    "        index_parts=True)\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "        'set this option by including two wetness thresholds in the `minimum_wet_thresholds` variable above'\n",
    "    )\n",
    "    merged_combined_polygons = cbd_filtered_primary_threshold_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff511f-587d-4c33-838e-733463076e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we do the river filtering (if filter_out_rivers == True).\n",
    "if filter_out_rivers:\n",
    "    major_rivers_filtered_polygons, intersect_indices = filter_shapefile_by_intersection(\n",
    "        merged_combined_polygons, major_rivers)\n",
    "else:\n",
    "    major_rivers_filtered_polygons = merged_combined_polygons\n",
    "\n",
    "major_rivers_filtered_polygons.crs = crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae450345-adef-4cff-ae22-594b08f1d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the area and perimeter of each polygon again now that overlapping polygons\n",
    "# have been merged.\n",
    "major_rivers_filtered_polygons['area'] = major_rivers_filtered_polygons[\n",
    "    'geometry'].area\n",
    "major_rivers_filtered_polygons['perimeter'] = major_rivers_filtered_polygons[\n",
    "    'geometry'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05856986-3b74-404f-9b18-577bcb348c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Polsby-Popper value (see below), and write out too.\n",
    "major_rivers_filtered_polygons['pp_test'] = (\n",
    "    (major_rivers_filtered_polygons['area'] * 4 * math.pi) /\n",
    "    (major_rivers_filtered_polygons['perimeter']**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d93e2-4ff5-4de3-933f-360b62b12295",
   "metadata": {},
   "source": [
    "\n",
    "### Dividing up very large polygons\n",
    "\n",
    "The size of polygons is determined by the contiguity of waterbody pixels through the landscape. This can result in very large polygons, e.g. where rivers are wide and unobscured by trees, or where waterbodies are connected to rivers or neighbouring waterbodies. \n",
    "\n",
    "We can break too large polygons into smaller, more useful polygons by applying the [Polsby-Popper test (1991)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2936284). The Polsby-Popper test is an assessment of the 'compactness' of a polygon. This method was originally developed to test the shape of congressional and state legislative districts, to prevent gerrymandering. \n",
    "\n",
    "The Polsby-Popper test examines the ratio between the area of a polygon, and the area of a circle equal to the perimeter of that polygon. The result falls between 0 and 1, with values closer to 1 being assessed as more compact.\n",
    "\n",
    "\\begin{align*}\n",
    "PPtest = \\frac{polygon\\ area * 4\\pi}{polygon\\ perimeter^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba1964-40dd-4df7-82ee-12c17a610e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dividing up very large polygons\n",
    "# this involves a bit of manual clean up.\n",
    "# The method to use to split very large polygons, by default 'nothing'\n",
    "# choose 'erode-dilate-v1', 'nothing' or 'erode-dilate-v2'.\n",
    "# handle_large_polygons = 'nothing'\n",
    "# Threshold to use when identifying and splitting very large polygons\n",
    "# using Polsby-Popper test, by default 0.005\n",
    "pp_thresh = 0.005\n",
    "\n",
    "if handle_large_polygons == 'erode-dilate-v1':\n",
    "    needs_buffer = major_rivers_filtered_polygons[\n",
    "        major_rivers_filtered_polygons.pp_test <= pp_thresh]\n",
    "    unbuffered = needs_buffer.buffer(-50)\n",
    "    unbuffered = unbuffered.explode(index_parts=True).reset_index(\n",
    "        drop=True).buffer(50)\n",
    "    unbuffered = gpd.GeoDataFrame(geometry=unbuffered,\n",
    "                                  crs=major_rivers_filtered_polygons.crs)\n",
    "    unbuffered['area'] = unbuffered.area\n",
    "    unbuffered['perimeter'] = unbuffered.length\n",
    "    unbuffered['pp_test'] = (unbuffered.area * 4 * math.pi /\n",
    "                             unbuffered.perimeter**2)\n",
    "    large_polygons_handled = pd.concat([\n",
    "        major_rivers_filtered_polygons[\n",
    "            major_rivers_filtered_polygons.pp_test > pp_thresh], unbuffered\n",
    "    ],\n",
    "                                       ignore_index=True)\n",
    "\n",
    "if handle_large_polygons == 'erode-dilate-v2':\n",
    "    splittable = major_rivers_filtered_polygons[\n",
    "        major_rivers_filtered_polygons.pp_test <= pp_thresh]\n",
    "    if len(splittable) >= 1:\n",
    "        unbuffered = splittable.buffer(-100)\n",
    "        buffered = unbuffered.buffer(125)\n",
    "        subtracted = gpd.overlay(\n",
    "            splittable,\n",
    "            gpd.GeoDataFrame(geometry=[buffered.unary_union],\n",
    "                             crs=splittable.crs),\n",
    "            how='difference').explode(index_parts=True).reset_index(drop=True)\n",
    "        resubtracted = gpd.overlay(\n",
    "            splittable, subtracted,\n",
    "            how='difference').explode(index_parts=True).reset_index(drop=True)\n",
    "\n",
    "        # Assign each chopped-off bit of the polygon to its nearest big\n",
    "        # neighbour.\n",
    "        unassigned = np.ones(len(subtracted), dtype=bool)\n",
    "        recombined = []\n",
    "        for i, poly in resubtracted.iterrows():\n",
    "            mask = (subtracted.exterior.intersects(poly.geometry.exterior) &\n",
    "                    unassigned)\n",
    "            neighbours = subtracted[mask]\n",
    "            unassigned[mask] = False\n",
    "            poly = poly.geometry.union(neighbours.unary_union)\n",
    "            recombined.append(poly)\n",
    "\n",
    "        # All remaining polygons are not part of a big polygon.\n",
    "        results = pd.concat([\n",
    "            gpd.GeoDataFrame(geometry=recombined,\n",
    "                             crs=major_rivers_filtered_polygons.crs),\n",
    "            subtracted[unassigned], major_rivers_filtered_polygons[\n",
    "                major_rivers_filtered_polygons.pp_test > pp_thresh]\n",
    "        ],\n",
    "                            ignore_index=True)\n",
    "\n",
    "        large_polygons_handled = results.explode(index_parts=True).reset_index(\n",
    "            drop=True)\n",
    "\n",
    "if handle_large_polygons == 'nothing':\n",
    "    large_polygons_handled = major_rivers_filtered_polygons\n",
    "    print('Not splitting large polygons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc1f48-ad1a-40b0-889e-d65d7bac946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the polygons to a shapefile\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {\n",
    "        'area': 'float',\n",
    "        'perimeter': 'float',\n",
    "        'PPtest': 'float'\n",
    "    }\n",
    "}\n",
    "\n",
    "output_fp = f'{waterbodies_shapefile_filtered}{file_extension}'\n",
    "\n",
    "with fiona.open(output_fp,\n",
    "                \"w\",\n",
    "                crs=crs,\n",
    "                driver=get_file_driver(output_fp),\n",
    "                schema=schema) as output:\n",
    "    for row in major_rivers_filtered_polygons.itertuples():\n",
    "        output.write(({\n",
    "            'properties': {\n",
    "                'area': row.area,\n",
    "                'perimeter': row.perimeter,\n",
    "                'PPtest': row.pp_test\n",
    "            },\n",
    "            'geometry': shapely.geometry.mapping(row.geometry)\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341074c-a532-420e-890a-565cf33ce5aa",
   "metadata": {},
   "source": [
    "### Final checks and recalculation of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213406c-e004-4e22-a832-b3fd3003914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_polygons = gpd.read_file(\n",
    "    f'{waterbodies_shapefile_filtered}{file_extension}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879010db-238b-49c7-89bc-2f2ecec068ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the area and perimeter of each polygon again following the manual checking\n",
    "# step performed above.\n",
    "filtered_polygons['area'] = filtered_polygons['geometry'].area\n",
    "filtered_polygons['perimeter'] = filtered_polygons['geometry'].length\n",
    "# Calculate the Polsby-Popper value (see below), and write out too.\n",
    "filtered_polygons['pp_test'] = (\n",
    "    (filtered_polygons['area'] * 4 * math.pi) /\n",
    "    (filtered_polygons['perimeter']**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3729612-24a9-48c2-b1b7-6908178f5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this for now to allow for further investigation when working with the different strategies\n",
    "# Remove the PPtest column, since we don't really want this as an attribute of the final shapefile.\n",
    "# filtered_polygons.drop(labels='PPtest', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee82b94-a28b-44a3-8bc5-89c042160ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply the size filtering, just to check that all of the split and filtered waterbodies are\n",
    "# still in the size range we want.\n",
    "filtered_polygons = filtered_polygons.loc[(\n",
    "    (filtered_polygons['area'] > min_polygon_size) &\n",
    "    (filtered_polygons['area'] <= max_polygon_size))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3842b73-2c8f-40a7-ab69-c4cd7807ebd5",
   "metadata": {},
   "source": [
    "### Generate a unique ID for each polygon\n",
    "\n",
    "A unique identifier is required for every polygon to allow it to be referenced. The naming convention for generating unique IDs here is the [geohash](geohash.org).\n",
    "\n",
    "A Geohash is a geocoding system used to generate short unique identifiers based on latitude/longitude coordinates. It is a short combination of letters and numbers, with the length of the string a function of the precision of the location. The methods for generating a geohash are outlined [here - yes, the official documentation is a wikipedia article](https://en.wikipedia.org/wiki/Geohash).\n",
    "\n",
    "Here we use the python package `python-geohash` to generate a geohash unique identifier for each polygon. We use `precision = 9` geohash characters, which represents an on the ground accuracy of <20 metres. This ensures that the precision is high enough to differentiate between waterbodies located next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c318e3-2692-4914-a728-075f087fe350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert to lat/lon, in order to generate the geohash.\n",
    "filtered_polygons_with_unique_ids = filtered_polygons.to_crs(epsg=4326)\n",
    "\n",
    "# Generate a geohash for the centroid of each polygon.\n",
    "filtered_polygons_with_unique_ids['UID'] = filtered_polygons_with_unique_ids.apply(lambda x: gh.encode(\n",
    "    x.geometry.centroid.y, x.geometry.centroid.x, precision=9),\n",
    "                                       axis=1)\n",
    "\n",
    "# Check that our unique ID is in fact unique\n",
    "assert filtered_polygons_with_unique_ids['UID'].is_unique\n",
    "\n",
    "# Make an arbitrary numerical ID for each polygon. We will first sort the dataframe by geohash\n",
    "# so that polygons close to each other are numbered similarly.\n",
    "filtered_polygons_with_unique_ids_sorted = filtered_polygons_with_unique_ids.sort_values(by=['UID']).reset_index()\n",
    "filtered_polygons_with_unique_ids_sorted['WB_ID'] = filtered_polygons_with_unique_ids_sorted.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5c454-f9db-4071-9240-4b30849f81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The step above creates an 'index' column, which we don't actually want, so drop it.\n",
    "filtered_polygons_with_unique_ids_sorted.drop(labels='index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784b15f-4051-48df-ba58-3550e6dd2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out final results to file.\n",
    "final_output_fp = f\"{waterbodies_shapefile_final}{file_extension}\"\n",
    "\n",
    "filtered_polygons_with_unique_ids_sorted = filtered_polygons_with_unique_ids_sorted.to_crs(crs)\n",
    "filtered_polygons_with_unique_ids_sorted.to_file(final_output_fp, driver=get_file_driver(final_output_fp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
