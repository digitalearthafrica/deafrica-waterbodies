{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0a31b-28d9-41d4-b59b-4ccb900e2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "import click\n",
    "import datacube\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skimage import measure, morphology\n",
    "from skimage.segmentation import watershed\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "from deafrica_tools.spatial import xr_vectorize\n",
    "\n",
    "from deafrica_waterbodies.cli.logs import logging_setup\n",
    "from deafrica_waterbodies.io import (\n",
    "    check_dir_exists,\n",
    "    check_file_exists,\n",
    "    check_if_s3_uri,\n",
    "    find_parquet_files,\n",
    ")\n",
    "from deafrica_waterbodies.make_polygons import (\n",
    "    check_wetness_thresholds,\n",
    "    get_polygons_from_tile_with_land_sea_mask_filtering,\n",
    "    merge_polygons_at_tile_boundaries\n",
    ")\n",
    "from deafrica_waterbodies.tiling import (\n",
    "    filter_tiles,\n",
    "    get_tiles_ids,\n",
    "    tile_wofs_ls_summary_alltime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c574ee-39c4-4931-b807-5d3f97900cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# These are the default AWS configurations for the Analysis Sandbox.\n",
    "# that are set in the environmnet variables.\n",
    "aws_default_config = {\n",
    "    # \"AWS_NO_SIGN_REQUEST\": \"YES\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\": \"fake\",\n",
    "    \"AWS_ACCESS_KEY_ID\": \"fake\",\n",
    "}\n",
    "\n",
    "# To access public bucket, need to remove the AWS credentials in\n",
    "# the environment variables or the following error will occur.\n",
    "# PermissionError: The AWS Access Key Id you provided does not exist in our records.\n",
    "\n",
    "for key in aws_default_config.keys():\n",
    "    if key in os.environ:\n",
    "        del os.environ[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50efa3-b5ba-44b3-b164-608d17e6413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "\n",
    "aoi_vector_file = None\n",
    "tile_size_factor = 4\n",
    "num_workers = 16\n",
    "\n",
    "primary_threshold: float = 0.1\n",
    "secondary_threshold: float = 0.05\n",
    "minimum_valid_observations: int = 128\n",
    "output_directory = \"s3://deafrica-waterbodies-dev/test_out_dir/raster_processing/continental\"\n",
    "overwrite = False\n",
    "land_sea_mask_fp = \"data/af_msk_3s.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a89fc-74fe-483b-89d5-b268c175d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "def filter_hydrosheds_land_mask(hydrosheds_land_mask: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Function to filter the HydroSHEDs Land Mask into a boolean mask.\n",
    "    \"\"\"\n",
    "    # Indicator values: 1 = land, 2 = ocean sink, 3 = inland sink, 255 is no data.\n",
    "    boolean_mask = (hydrosheds_land_mask != 255) & (hydrosheds_land_mask != 2)\n",
    "    return boolean_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfbcc2-fd39-4900-815c-09a11c448b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger.\n",
    "logging_setup(verbose=verbose)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0742d7-6ca2-4286-9313-3efda5173f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support pathlib Paths.\n",
    "if aoi_vector_file is not None:\n",
    "    aoi_vector_file = str(aoi_vector_file)\n",
    "output_directory = str(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db4ad8-7193-47e4-a4b4-44085924a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to use when loading datasets.\n",
    "dask_chunks = {\"x\": 3200, \"y\": 3200, \"time\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0ca66-2695-42c8-968a-523151d514c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the area of interest as a GeoDataFrame.\n",
    "if aoi_vector_file is not None:\n",
    "    try:\n",
    "        aoi_gdf = gpd.read_file(aoi_vector_file)\n",
    "    except Exception as error:\n",
    "        _log.exception(f\"Could not read the file {aoi_vector_file}\")\n",
    "        raise error\n",
    "else:\n",
    "    aoi_gdf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b4349-4d9c-4e55-aae0-b661ccde8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile the wofs_ls_summary_alltime product.\n",
    "tiles, grid_workflow = tile_wofs_ls_summary_alltime(tile_size_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134e90d-9c9a-47ca-86da-687846f2568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tiles to the area of interest.\n",
    "filtered_tile_ids = filter_tiles(tiles, aoi_gdf, num_workers)\n",
    "filtered_tiles = {k: v for k, v in tiles.items() if k in filtered_tile_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73816db0-7445-423a-a616-9966187a0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to write generated waterbody polygons to.\n",
    "polygons_from_thresholds_dir = os.path.join(output_directory, \"polygons_from_thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a0c8b-9bc5-469e-8bc2-9a2a0588265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the filesystem to use.\n",
    "if check_if_s3_uri(polygons_from_thresholds_dir):\n",
    "    fs = fsspec.filesystem(\"s3\")\n",
    "else:\n",
    "    fs = fsspec.filesystem(\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe40837-f598-4d7c-a3fd-6c1deb2b0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory exists. If it does not, create it.\n",
    "if not check_dir_exists(polygons_from_thresholds_dir):\n",
    "    fs.mkdirs(polygons_from_thresholds_dir, exist_ok=True)\n",
    "    _log.info(f\"Created directory {polygons_from_thresholds_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85a8d3-9543-4a57-90b8-3ac5e6cae71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the wetness thresholds have been set correctly.\n",
    "minimum_wet_thresholds = [secondary_threshold, primary_threshold]\n",
    "_log.info(check_wetness_thresholds(minimum_wet_thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40837a2-5fd5-42a4-9905-55d72313e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wofs_frequency(\n",
    "    tile: tuple[tuple[int, int], datacube.api.grid_workflow.Tile],\n",
    "    grid_workflow: datacube.api.GridWorkflow,\n",
    "    dask_chunks: dict[str, int] = {\"x\": 3200, \"y\": 3200, \"time\": 1},\n",
    "    resolution: tuple[int, int] = (-30, 30),\n",
    "    output_crs: str = \"EPSG:6933\",\n",
    "    min_valid_observations: int = 128,\n",
    "    primary_threshold: float = 0.1,\n",
    "    secondary_threshold: float = 0.05,\n",
    "    land_sea_mask_fp: str | Path = \"\",\n",
    "    resampling_method: str = \"bilinear\",\n",
    "    filter_land_sea_mask: Callable = filter_hydrosheds_land_mask,\n",
    "):\n",
    "    # Set up the primary and secondary thresholds.\n",
    "    minimum_wet_thresholds = [secondary_threshold, primary_threshold]\n",
    "    \n",
    "    # Get the tile id and tile object.\n",
    "    tile_id = tile[0]\n",
    "    tile_object = tile[1]\n",
    "    \n",
    "    # Generate the waterbody polygons using the primary and secondary thresholds,\n",
    "    # from the tile.\n",
    "    try:\n",
    "        _log.info(f\"Generating water body polygons for tile {tile_id}\")\n",
    "\n",
    "        # Load the data for the tile.\n",
    "        wofs_alltime_summary = grid_workflow.load(tile_object, dask_chunks=dask_chunks).squeeze()\n",
    "\n",
    "        # Load the land sea mask.\n",
    "        if land_sea_mask_fp:\n",
    "            land_sea_mask = rio_slurp_xarray(\n",
    "                fname=land_sea_mask_fp,\n",
    "                gbox=wofs_alltime_summary.geobox,\n",
    "                resampling=resampling_method,\n",
    "            )\n",
    "\n",
    "            # Filter the land sea mask.\n",
    "            boolean_land_sea_mask = filter_land_sea_mask(land_sea_mask)\n",
    "\n",
    "            # Mask the WOfS All-Time Summary dataset using the boolean land sea mask.\n",
    "            wofs_alltime_summary = wofs_alltime_summary.where(boolean_land_sea_mask)\n",
    "\n",
    "        # Set the no-data values to nan.\n",
    "        # Masking here is done using the frequency measurement because for multiple\n",
    "        # areas NaN values are present in the frequency measurement but the\n",
    "        # no data value -999 is not present in the count_clear and\n",
    "        # count_wet measurements.\n",
    "        # Note: it seems some pixels with NaN values in the frequency measurement\n",
    "        # have a value of zero in the count_clear and/or the count_wet measurements.\n",
    "        wofs_alltime_summary = wofs_alltime_summary.where(~np.isnan(wofs_alltime_summary.frequency))\n",
    "\n",
    "        # Mask pixels not observed at least min_valid_observations times.\n",
    "        wofs_alltime_summary_valid_clear_count = (\n",
    "            wofs_alltime_summary.count_clear >= min_valid_observations\n",
    "        )\n",
    "        \n",
    "        # Get detection and extent thresholds\n",
    "        detection = wofs_alltime_summary.frequency > primary_threshold\n",
    "        valid_detection = (detection > 0) & wofs_alltime_summary_valid_clear_count\n",
    "        \n",
    "        extent = wofs_alltime_summary.frequency > secondary_threshold\n",
    "        valid_extent = (extent > 0) & wofs_alltime_summary_valid_clear_count\n",
    "            \n",
    "    except Exception as error:\n",
    "        _log.exception(\n",
    "            f\"\\nDataset {str(tile_id)} did not run. \\n\"\n",
    "            \"This is probably because there are no waterbodies present in this scene.\"\n",
    "        )\n",
    "        _log.exception(error)\n",
    "\n",
    "    return valid_detection, valid_extent\n",
    "\n",
    "def remove_small_waterbodies(waterbody_raster, min_size=6):\n",
    "    \n",
    "    waterbodies_labelled = morphology.label(waterbody_raster, background=0)\n",
    "    waterbodies_small_removed = morphology.remove_small_objects(waterbodies_labelled, min_size=min_size, connectivity=1)\n",
    "    \n",
    "    return waterbodies_small_removed\n",
    "\n",
    "# Need a step to only segment the largest objects\n",
    "# only segment bigger than minsize\n",
    "def select_waterbodies_for_segmentation(waterbodies_labelled, min_size=1000):\n",
    "    props = measure.regionprops(waterbodies_labelled)\n",
    "    \n",
    "    labels_to_keep = []\n",
    "    for region_prop in props:\n",
    "        count = region_prop.num_pixels\n",
    "        label = region_prop.label\n",
    "        \n",
    "        if (count > min_size):\n",
    "            labels_to_keep.append(label)\n",
    "            \n",
    "    segment_image = np.where(np.isin(waterbodies_labelled, labels_to_keep), 1, 0)\n",
    "    \n",
    "    return segment_image\n",
    "\n",
    "def generate_segmentation_markers(marker_source, erosion_radius=1, min_size=100):\n",
    "    \n",
    "    markers = morphology.erosion(marker_source, footprint=morphology.disk(radius=erosion_radius))\n",
    "    markers_relabelled = morphology.label(markers, background=0)\n",
    "    \n",
    "    markers_acceptable_size = morphology.remove_small_objects(markers_relabelled, min_size=min_size, connectivity=1)\n",
    "    \n",
    "    return markers_acceptable_size\n",
    "\n",
    "def run_watershed(waterbodies_for_segementation, segmentation_markers):\n",
    "    \n",
    "    distance = ndi.distance_transform_edt(waterbodies_for_segementation)\n",
    "    segmented = watershed(-distance, segmentation_markers, mask=waterbodies_for_segementation)\n",
    "    \n",
    "    return segmented\n",
    "\n",
    "def confirm_extent_contains_detection(extent, detection):\n",
    "    \n",
    "    def sum_intensity(regionmask, intensity_image):\n",
    "        return np.sum(intensity_image[regionmask])\n",
    "    \n",
    "    props = measure.regionprops(extent, intensity_image=detection, extra_properties=(sum_intensity,))\n",
    "    \n",
    "    labels_to_keep = []\n",
    "    for region_prop in props:\n",
    "        detection_count = region_prop.sum_intensity\n",
    "        label = region_prop.label\n",
    "        \n",
    "        if (detection_count > 0):\n",
    "            labels_to_keep.append(label)\n",
    "            \n",
    "    extent_keep = np.where(np.isin(extent, labels_to_keep), extent, 0)\n",
    "    \n",
    "    return extent_keep\n",
    "\n",
    "def process_raster_polygons(\n",
    "    tile: tuple[tuple[int, int], datacube.api.grid_workflow.Tile],\n",
    "    grid_workflow: datacube.api.GridWorkflow,\n",
    "    dask_chunks: dict[str, int] = {\"x\": 3200, \"y\": 3200, \"time\": 1},\n",
    "    resolution: tuple[int, int] = (-30, 30),\n",
    "    output_crs: str = \"EPSG:6933\",\n",
    "    min_valid_observations: int = 128,\n",
    "    primary_threshold: float = 0.1,\n",
    "    secondary_threshold: float = 0.05,\n",
    "    dc: datacube.Datacube | None = None,\n",
    "    land_sea_mask_fp: str | Path = \"\",\n",
    "    filter_land_sea_mask: Callable = filter_hydrosheds_land_mask,\n",
    "): \n",
    "    \n",
    "    xr_detection, xr_extent = load_wofs_frequency(tile, grid_workflow, land_sea_mask_fp=land_sea_mask_fp, filter_land_sea_mask=filter_hydrosheds_land_mask)\n",
    "\n",
    "    # Convert to numpy arrays for image processing\n",
    "    np_detection = xr_detection.to_numpy().astype(int)\n",
    "    np_extent = xr_extent.to_numpy().astype(int)\n",
    "\n",
    "    # Remove any objects of size 5 or less, as measured by connectivity=1\n",
    "    np_extent_small_removed = remove_small_waterbodies(np_extent, min_size=6)\n",
    "\n",
    "    # Identify waterbodies to apply segmentation to \n",
    "    np_extent_segment = select_waterbodies_for_segmentation(np_extent_small_removed, min_size=1000)\n",
    "    np_extent_nosegment = np.where(np_extent_segment>0, 0, np_extent_small_removed)\n",
    "\n",
    "    # Create watershed segementation markers by taking the detection threshold pixels and eroding them by 1\n",
    "    # Includes removal of any markers smaller than 100 pixels\n",
    "    segmentation_markers = generate_segmentation_markers(np_detection, erosion_radius=1, min_size=100)\n",
    "\n",
    "    # Run segmentation \n",
    "    np_segmented_extent = run_watershed(np_extent_segment, segmentation_markers)\n",
    "\n",
    "    # Combine segmented and non segmented back together\n",
    "    np_combined_extent = np.where(np_segmented_extent > 0, np_segmented_extent, np_extent_nosegment)\n",
    "\n",
    "    # Only keep extent areas that contain a detection pixel\n",
    "    np_combined_extent_contains_detection = confirm_extent_contains_detection(np_combined_extent, np_detection)\n",
    "\n",
    "    # Relabel and remove small objects\n",
    "    np_combined_clean_label = remove_small_waterbodies(np_combined_extent_contains_detection, min_size=6)\n",
    "\n",
    "    # Convert back to xarray\n",
    "    xr_combined_extent = xr.DataArray(np_combined_clean_label, coords=xr_extent.coords, dims=xr_extent.dims, attrs=xr_extent.attrs)\n",
    "\n",
    "    # Vectorize\n",
    "    vector_combined_extent = xr_vectorize(xr_combined_extent, crs=output_crs, mask=xr_combined_extent.values>0)\n",
    "    \n",
    "    return vector_combined_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f37f02-ce60-43c9-9dc4-4c5298b15a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the first set of primary and secondary threhsold polygons for each of the tiles.\n",
    "for tile in filtered_tiles.items():\n",
    "    tile_id = tile[0]\n",
    "    \n",
    "    raster_polygons_fp = os.path.join(polygons_from_thresholds_dir, f\"{tile_id[0]}_{tile_id[1]}_raster_polygons.parquet\")\n",
    "\n",
    "    if not overwrite:\n",
    "        _log.info(f\"Checking existence of {raster_polygons_fp}\")\n",
    "        exists = check_file_exists(raster_polygons_fp)\n",
    "\n",
    "    if overwrite or not exists:\n",
    "        \n",
    "        try: \n",
    "            raster_polgyons = process_raster_polygons(tile, grid_workflow, land_sea_mask_fp=land_sea_mask_fp, filter_land_sea_mask=filter_hydrosheds_land_mask)\n",
    "            \n",
    "             # Write the polygons to parquet files.\n",
    "            raster_polgyons.to_parquet(raster_polygons_fp)\n",
    "            \n",
    "        except Exception as error:\n",
    "            _log.exception(\n",
    "                f\"\\nDataset {str(tile_id)} did not run. \\n\"\n",
    "            )\n",
    "            _log.exception(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d552b8-32fd-427c-835b-92245f627d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extents for each tile.\n",
    "crs = grid_workflow.grid_spec.crs\n",
    "filtered_tiles_extents_geoms = [tile[1].geobox.extent.geom for tile in filtered_tiles.items()]\n",
    "filtered_tiles_extents_gdf = gpd.GeoDataFrame(geometry=filtered_tiles_extents_geoms, crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394ffbb-bf75-4cc8-9c1c-09212772419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all parquet files for the primary threshold.\n",
    "raster_polygon_paths = find_parquet_files(path=polygons_from_thresholds_dir, pattern=\".*raster_polygons.*\")\n",
    "_log.info(f\"Found {len(raster_polygon_paths)} parquet files for the raster polygons.\")\n",
    "\n",
    "# Load all the primary threshold polygons into a single GeoDataFrame.\n",
    "_log.info(\"Loading the raster polygons parquet files..\")\n",
    "raster_polygon_polygons_list = []\n",
    "for path in raster_polygon_paths:\n",
    "    gdf = gpd.read_parquet(path)\n",
    "    raster_polygon_polygons_list.append(gdf)\n",
    "\n",
    "raster_polygons = pd.concat(raster_polygon_polygons_list, ignore_index=True)\n",
    "_log.info(f\"Found {len(raster_polygons)} raster polygons.\")\n",
    "\n",
    "_log.info(\"Merging raster waterbody polygons located at tile boundaries...\")\n",
    "raster_polygons_merged = merge_polygons_at_tile_boundaries(\n",
    "    raster_polygons, filtered_tiles_extents_gdf\n",
    ")\n",
    "_log.info(f\"Raster polygons count {len(raster_polygons_merged)}.\")\n",
    "\n",
    "_log.info(\"Writing raster polygons merged at tile boundaries to disk..\")\n",
    "raster_polygons_output_fp = os.path.join(\n",
    "    output_directory, \"raster_polygons_merged_at_tile_boundaries.parquet\"\n",
    ")\n",
    "\n",
    "raster_polygons_merged.to_parquet(raster_polygons_output_fp)\n",
    "_log.info(f\"Polygons written to {raster_polygons_output_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0c48e-feeb-47c0-a13b-fbe3ee940ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all parquet files for the primary threshold.\n",
    "# primary_threshold_polygons_paths = find_parquet_files(path=polygons_from_thresholds_dir, pattern=\".*primary.*\")\n",
    "# _log.info(f\"Found {len(primary_threshold_polygons_paths)} parquet files for the primary threshold polygons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb0c77-93d9-4cc6-a2d2-2542875abf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all the primary threshold polygons into a single GeoDataFrame.\n",
    "# _log.info(\"Loading the primary threshold polygons parquet files..\")\n",
    "# primary_threshold_polygons_list = []\n",
    "# for path in primary_threshold_polygons_paths:\n",
    "#     gdf = gpd.read_parquet(path)\n",
    "#     primary_threshold_polygons_list.append(gdf)\n",
    "\n",
    "# primary_threshold_polygons = pd.concat(primary_threshold_polygons_list, ignore_index=True)\n",
    "# _log.info(f\"Found {len(primary_threshold_polygons)} primary threshold polygons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fa531-851c-47f9-a9d4-87647b130da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _log.info(\"Merging primary threshold waterbody polygons located at tile boundaries...\")\n",
    "# primary_threshold_polygons_merged = merge_polygons_at_tile_boundaries(\n",
    "#     primary_threshold_polygons, filtered_tiles_extents_gdf\n",
    "# )\n",
    "# _log.info(f\"Primary threshold polygons count {len(primary_threshold_polygons_merged)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70430dbd-8ef5-46cb-8def-c9cf5c0dda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _log.info(\"Writing primary threshold polygons merged at tile boundaries to disk..\")\n",
    "# primary_threshold_polygons_output_fp = os.path.join(\n",
    "#     output_directory, \"primary_threshold_polygons_merged_at_tile_boundaries.parquet\"\n",
    "# )\n",
    "\n",
    "# primary_threshold_polygons_merged.to_parquet(primary_threshold_polygons_output_fp)\n",
    "# _log.info(f\"Polygons written to {primary_threshold_polygons_output_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70440491-216a-42d6-b26e-c943b918a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all parquet files for the secondary threshold.\n",
    "# secondary_threshold_polygons_paths = find_parquet_files(path=polygons_from_thresholds_dir, pattern=\".*secondary.*\")\n",
    "# _log.info(f\"Found {len(secondary_threshold_polygons_paths)} parquet files for the secondary threshold polygons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b45e5-caf9-4df4-8d36-b96cfb733d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all the secondary threshold polygons into a single GeoDataFrame.\n",
    "# _log.info(\"Loading the secondary threshold polygons parquet files...\")\n",
    "# secondary_threshold_polygons_list = []\n",
    "# for path in secondary_threshold_polygons_paths:\n",
    "#     gdf = gpd.read_parquet(path)\n",
    "#     secondary_threshold_polygons_list.append(gdf)\n",
    "\n",
    "# secondary_threshold_polygons = pd.concat(secondary_threshold_polygons_list, ignore_index=True)\n",
    "# _log.info(f\"Found {len(secondary_threshold_polygons)} secondary threshold polygons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010dca0-9357-4a94-936c-b36e3500e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _log.info(\"Merging secondary threshold waterbody polygons located at dataset/scene boundaries...\")\n",
    "# secondary_threshold_polygons_merged = merge_polygons_at_tile_boundaries(\n",
    "#     secondary_threshold_polygons, filtered_tiles_extents_gdf\n",
    "# )\n",
    "# _log.info(f\"Secondary threshold polygons count {len(secondary_threshold_polygons_merged)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703dc6e4-8ce0-48a3-851b-22355daed66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _log.info(\"Writing secondary threshold polygons merged at tile boundaries to disk..\")\n",
    "# secondary_threshold_polygons_output_fp = os.path.join(\n",
    "#     output_directory, \"secondary_threshold_polygons_merged_at_ds_boundaries.parquet\"\n",
    "# )\n",
    "\n",
    "# secondary_threshold_polygons_merged.to_parquet(secondary_threshold_polygons_output_fp)\n",
    "\n",
    "# _log.info(f\"Polygons written to {secondary_threshold_polygons_output_fp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
