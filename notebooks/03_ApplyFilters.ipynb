{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdabd764-0960-4bcd-84f5-e007852c3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import click\n",
    "import geopandas as gpd\n",
    "from deafrica_waterbodies.cli.logs import logging_setup\n",
    "from deafrica_waterbodies.filters import (\n",
    "    filter_by_area,\n",
    "    filter_using_land_sea_mask,\n",
    "    filter_using_major_rivers_mask,\n",
    "    filter_using_urban_mask,\n",
    "    merge_primary_and_secondary_threshold_polygons,\n",
    "    split_large_polygons,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af67100-ff19-440f-b6e5-fe5724244ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# These are the default AWS configurations for the Analysis Sandbox.\n",
    "# that are set in the environmnet variables.\n",
    "aws_default_config = {\n",
    "    # \"AWS_NO_SIGN_REQUEST\": \"YES\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\": \"fake\",\n",
    "    \"AWS_ACCESS_KEY_ID\": \"fake\",\n",
    "}\n",
    "\n",
    "# To access public bucket, need to remove the AWS credentials in\n",
    "# the environment variables or the following error will occur.\n",
    "# PermissionError: The AWS Access Key Id you provided does not exist in our records.\n",
    "\n",
    "for key in aws_default_config.keys():\n",
    "    if key in os.environ:\n",
    "        del os.environ[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc8246-b844-4e92-bc15-204bb54702e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "output_directory = \"s3://deafrica-waterbodies-dev/test_out_dir/raster_processing/continental\"\n",
    "min_polygon_size = 4500  # 5 pixels\n",
    "max_polygon_size = math.inf\n",
    "land_sea_mask_fp = \"\"\n",
    "major_rivers_mask_fp = \"\"\n",
    "urban_mask_fp = \"\"\n",
    "handle_large_polygons = \"erode_dialate_v2\"\n",
    "pp_test_threshold = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a30849-c86a-4b8a-947c-cc12245d618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger.\n",
    "logging_setup(verbose=verbose)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670f7c9-b5bf-4c41-b1c7-150da3728985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support pathlib paths.\n",
    "output_directory = str(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0e9e0-2af9-473c-bdd2-65b4d64032f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the primary and secondary threshold polygons\n",
    "_log.info(\"Loading primary and secondary threshold polygons...\")\n",
    "\n",
    "primary_threshold_polygons_fp = os.path.join(\n",
    "    output_directory, \"primary_threshold_polygons_merged_at_ds_boundaries.parquet\"\n",
    ")\n",
    "secondary_threshold_polygons_fp = os.path.join(\n",
    "    output_directory, \"secondary_threshold_polygons_merged_at_ds_boundaries.parquet\"\n",
    ")\n",
    "primary_threshold_polygons = gpd.read_parquet(primary_threshold_polygons_fp)\n",
    "secondary_threshold_polygons = gpd.read_parquet(secondary_threshold_polygons_fp)\n",
    "\n",
    "_log.info(f\"Primary threshold polygons count {len(primary_threshold_polygons)}.\")\n",
    "_log.info(f\"Secondary threshold polygons count {len(secondary_threshold_polygons)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08d600-b0ab-41c8-a2fa-43a6015609d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    area_filtered_primary_threshold_polygons,\n",
    "    area_filtered_secondary_threshold_polygons,\n",
    ") = filter_by_area(\n",
    "    primary_threshold_polygons=primary_threshold_polygons,\n",
    "    secondary_threshold_polygons=secondary_threshold_polygons,\n",
    "    min_polygon_size=min_polygon_size,\n",
    "    max_polygon_size=max_polygon_size,\n",
    ")\n",
    "\n",
    "area_filtered_primary_threshold_polygons_fp = os.path.join(\n",
    "    output_directory, \"area_filtered_primary_threshold_polygons.parquet\"\n",
    ")\n",
    "area_filtered_primary_threshold_polygons.to_parquet(area_filtered_primary_threshold_polygons_fp)\n",
    "_log.info(f\"Area filtered primary threshold polygons written to {area_filtered_primary_threshold_polygons_fp}\")\n",
    "\n",
    "area_filtered_secondary_threshold_polygons_fp = os.path.join(\n",
    "    output_directory, \"area_filtered_secondary_threshold_polygons.parquet\"\n",
    ")\n",
    "area_filtered_secondary_threshold_polygons.to_parquet(area_filtered_secondary_threshold_polygons_fp)\n",
    "_log.info(f\"Area filtered secondary threshold polygons written to {area_filtered_secondary_threshold_polygons_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18c51c-3e1a-44a9-80ce-17fcce0422fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    inland_primary_threshold_polygons,\n",
    "    inland_secondary_threshold_polygons,\n",
    ") = filter_using_land_sea_mask(\n",
    "    primary_threshold_polygons=area_filtered_primary_threshold_polygons,\n",
    "    secondary_threshold_polygons=area_filtered_secondary_threshold_polygons,\n",
    "    land_sea_mask_fp=land_sea_mask_fp,\n",
    ")\n",
    "\n",
    "inland_primary_threshold_polygons_fp = os.path.join(output_directory, \"inland_primary_threshold_polygons.parquet\")\n",
    "inland_primary_threshold_polygons.to_parquet(inland_primary_threshold_polygons_fp)\n",
    "_log.info(f\"Ocean filtered primary threshold polygons written to {inland_primary_threshold_polygons_fp}\")\n",
    "\n",
    "inland_secondary_threshold_polygons_fp = os.path.join(output_directory, \"inland_secondary_threshold_polygons.parquet\")\n",
    "inland_secondary_threshold_polygons.to_parquet(inland_secondary_threshold_polygons_fp)\n",
    "_log.info(f\"Ocean filtered secondary threshold polygons written to {inland_secondary_threshold_polygons_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52b2bf-5fda-4eec-bddd-9603fb3775dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cbd_filtered_primary_threshold_polygons,\n",
    "    cbd_filtered_secondary_threshold_polygons,\n",
    ") = filter_using_urban_mask(\n",
    "    primary_threshold_polygons=inland_primary_threshold_polygons,\n",
    "    secondary_threshold_polygons=inland_secondary_threshold_polygons,\n",
    "    urban_mask_fp=urban_mask_fp,\n",
    ")\n",
    "\n",
    "\n",
    "cbd_filtered_primary_threshold_polygons_fp = os.path.join(\n",
    "    output_directory, \"cbd_filtered_primary_threshold_polygons.parquet\"\n",
    ")\n",
    "cbd_filtered_primary_threshold_polygons.to_parquet(cbd_filtered_primary_threshold_polygons_fp)\n",
    "_log.info(f\"CBDs filtered primary threshold polygons written to {cbd_filtered_primary_threshold_polygons_fp}\")\n",
    "\n",
    "cbd_filtered_secondary_threshold_polygons_fp = os.path.join(\n",
    "    output_directory, \"cbd_filtered_secondary_threshold_polygons.parquet\"\n",
    ")\n",
    "cbd_filtered_secondary_threshold_polygons.to_parquet(cbd_filtered_secondary_threshold_polygons_fp)\n",
    "_log.info(f\"CBDs filtered secondary threshold polygons written to {cbd_filtered_secondary_threshold_polygons_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e3f21-228c-4856-b065-50157421bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Merge the primary and secondary threshold polygons.\n",
    "merged_polygons = merge_primary_and_secondary_threshold_polygons(\n",
    "    primary_threshold_polygons=cbd_filtered_primary_threshold_polygons,\n",
    "    secondary_threshold_polygons=cbd_filtered_secondary_threshold_polygons,\n",
    ")\n",
    "\n",
    "merged_polygons_fp = os.path.join(output_directory, \"merged_polygons.parquet\")\n",
    "merged_polygons.to_parquet(merged_polygons_fp)\n",
    "_log.info(f\"Merged waterbody polygons written to {merged_polygons_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4dc2a-e6d6-46f2-becf-912802d68344",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_rivers_filtered_polygons = filter_using_major_rivers_mask(\n",
    "    waterbody_polygons=merged_polygons, major_rivers_mask_fp=major_rivers_mask_fp\n",
    ")\n",
    "\n",
    "major_rivers_filtered_polygons_fp = os.path.join(output_directory, \"major_rivers_filtered_polygons.parquet\")\n",
    "major_rivers_filtered_polygons.to_parquet(major_rivers_filtered_polygons_fp)\n",
    "_log.info(f\"Major rivers filtered polygons written to {major_rivers_filtered_polygons_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f56b2e-b820-41cb-a8ef-5ba96291d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle large polygons.\n",
    "large_polygons_handled = split_large_polygons(\n",
    "    waterbody_polygons=major_rivers_filtered_polygons, pp_thresh=pp_test_threshold, method=handle_large_polygons\n",
    ")\n",
    "_log.info(f\"Waterbody polygons count after splitting large polygons {len(large_polygons_handled)}.\")\n",
    "\n",
    "large_polygons_handled_fp = os.path.join(output_directory, f\"large_polygons_handled_{handle_large_polygons}.parquet\")\n",
    "large_polygons_handled.to_parquet(large_polygons_handled_fp)\n",
    "_log.info(f\"Waterbodies with large polygons handled written to {large_polygons_handled_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e4da4-ba78-48bb-85c6-c4e044cc5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply the size filtering, just to check that all of the split and filtered waterbodies are\n",
    "# still in the size range we want.\n",
    "area_filtered_large_polygons_handled, _ = filter_by_area(\n",
    "    primary_threshold_polygons=large_polygons_handled,\n",
    "    secondary_threshold_polygons=None,\n",
    "    min_polygon_size=min_polygon_size,\n",
    "    max_polygon_size=max_polygon_size,\n",
    ")\n",
    "\n",
    "area_filtered_large_polygons_handled_fp = os.path.join(output_directory, \"area_filtered_large_polygons_handled.parquet\")\n",
    "area_filtered_large_polygons_handled.to_parquet(area_filtered_large_polygons_handled_fp)\n",
    "_log.info(f\"Area filtered polygons written to {area_filtered_large_polygons_handled_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fd892-24d0-491b-873e-05ebefb3014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a GeoDataFrame with the geometry column only.\n",
    "filtered_polygons = gpd.GeoDataFrame(\n",
    "    geometry=area_filtered_large_polygons_handled[\"geometry\"], crs=area_filtered_large_polygons_handled.crs\n",
    ")\n",
    "filtered_polygons_fp = os.path.join(output_directory, \"filtered_polygons.parquet\")\n",
    "filtered_polygons.to_parquet(filtered_polygons_fp)\n",
    "_log.info(f\"Filtered waterbody polygons written to {filtered_polygons_fp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
